= Match Compilation [110.97 version]
:Author: Dave MacQueen
:Date: 2020/05/09
:stem: latexmath
:source-highlighter: pygments
:VERSION: 110.98

== [1] Introduction

This document describes the revised implementation of the match compiler
SML/NJ 110.98. This document replaces the Alex Aiken compiler note.

== [2] Old Files (in base/compiler/FLINT/trans)
   mcommon.sml    -- match compiler types
   matchcomp.sml  -- the main (or reset of the) match compiler
   translate.sml  -- old translate file, calling match compiler functions

== [3] New Files (in base/compiler/Elaborator/matchcomp)
   mctypes.sml  -- match compiler types (replaces trans/mccommon.sml)
                   defines types key, path, andor, dec-tree, and mcexp
   rules.{sig,sml}
                -- ruleno sets (sets of rule numbers)
   andor.sml    -- construction or the andor tree from pattern list
   decisiontree.sml
                -- constructing a decision tree from andor tree,
   		   decision trees involve only OR nodes
   mcexp.sml    -- concrete abstract syntax of match compiler expressions
   vmcexp.sml   -- virtual implementation of mcexp in terms of Absyn.exp
   mc-code.sml  -- constructing "abstract" match compiler code (mcexp)
                   from rules, andor, and dec-tree for a match
   vcode.sml    -- translating "virtual" mcexp expressions to Absyn
   match.sml    -- top-level matchComp function
   transmc.sml  -- translating typed Absyn into Absyn with compiled matches
 
== [4] Final Files
   mctypes.sml  -- match compiler types (replaces trans/mccommon.sml)
                   defines types key, path, andor, dec-tree, and mcexp
   rules.{sig,sml}
                -- ruleno sets (sets of rule numbers)
   andor.sml    -- construction or the andor tree from pattern list
   decisiontree.sml
                -- constructing a decision tree from andor tree,
   		   decision trees involve only OR nodes
   matchcomp.sml -- match compilation during absyn to absyn translation
   svarenv.sml   -- ??
   varenvmc.sml  -- environment used for match compilation
   varenvac.sml  -- environment used for alpha-conversion

== [4] Terminology anb basic types

The types used in the match compiler are defined 

- type ruleno = int
   index, zero-based, of a rule in a match
   [OBS] E.g. "allRules" defined in matchComp, which is the list after OR expansion

- type ruleset = ruleno list = int list
   an ordered list of rule indices (no duplicates) representing rule index sets
   Based on BinaryIntMap library (smlnj-lib/Util) module.
   
- type dconinfo = datacon * tyvar list
   [OBS] a dataconstructor (occurring in a pattern) with an "associated"? list of tyvars
   [Q: what is the relation between the datacon and the tyvars?]

- "destructuring paths" (datatype path)
  [OBS: paths that record a position in a decision tree? But paths are _trees_ because
   of the RECORDPATH constructor! ]

=== [5] Two complications in the and-or analysis.

- layered patterns. But the pattern that is layered over a base pattern is
  restricted to be a variable (possibly with at type constraint), so the "information
  content is not modified by a layered pattern. It just introduces a variable
  binding at that node.

- vector patterns. These are _both_ OR (dispatching on the length of the vector)
  and AND in the sense that all the element patterns of the vector pattern must
  be matched.  These two roles could possibly be separated, splitting the node
  into an OR over lengths, and for each 

   case x 
     of #[nil, x::y] => ...
      | #[z::w, nil] => ...

  Here the two patterns are vectors of the same length, but the element patterns
  for OR choices. This case should be analyzed similarly to

   case x 
     of {a = nil, b = x::y} => ...
      | {a = z::w, b = nil} => ...

  where on the "a"-field there is an OR choice on {nil, z::w} and on the "b"-field
  there is an OR choice on {x::y, nil}. But we know (from type-checking} that all
  record patterns at the same node will have the same record type and hence the same
  fields.  So there is no OR choice at the level of the record patterns themselves,
  only at the level of corresponding field patterns, where in this case, both the
  "a"-field and "b"-field give rise to OR choices (which will have to be ordered 
  during the construction of the decision tree).

  But two vector patterns at the same node may be of different lengths:

      case x 
	 of #[nil, x::y] => ...
	  | #[z::w, nil, u] => ...

  Here we do the OR dispatch solely on the vector length, and the respective field
  patterns (here {nil, z::w} for the 1st element) do not constitute an OR choice --
  they are "independent" because they lie in separate OR branches.  This is analagous
  to the following situation:

     datatype t = A of int list * int list
                | B of int list * int list

     case x
       of A(nil,x::y) =>
        | B(z::w,nil) =>

  where we don't dispatch on the {nil, z::w} patterns because the A vs B OR node
  has separated them into two branches.

== [6] And-Or trees

Principles:

- Pattern space (for a type)
  Types are either pattern opaque (atomic) or pattern transparent:

  (1) pattern opaque types are types that are atomic or opaque wrt pattern matching,
      i.e. they cannot be analyzed or deconstructed using pattern matching. 
      These types are function types, abstract types, and primitive types other
      than integers, words, characters and strings. There is, not surprisingly,
      a lot of overlap with "equality" types (types that admit generic equality).

  (2) pattern transparent types are formed by products and sums (datatypes) and
      recursion, plus a small set of primitive types that can match atomic (constant)
      patterns, namely int, word, char, and string types.

Given a pattern-transparent type (pattern type, for short), it can be
unfolded into an infinite (or quasi-infinite, in the case of a
primitive type like int) tree constructed from AND nodes (for
products) and OR nodes for sums. [E.g. infinite (regular) tree for 'a
list.]

A sequence of patterns for a pattern type determines a finite "initial
segment" of the tree for the pattern type. The branches of this initial
tree (the union of the finite branches (paths) in the individual patterns,
is called the pattern spece for the pattern set. The branches are finite and
terminate with (a) atomic variable patterns, or (b) constant patterns, where
a constant can be a constant for a primitive type (e.g. 3) or a constant data
constructor (e.g. true, false, nil).

This is the basic AND-OR tree for the pattern sequence, and it is a concrete 
representation of the pattern space generated by the pattern sequence.

[7] Compilications (for SML)

(1) layered patterns of the form "x as p".
These introduce variable binding points at intermediate nodes in the pattern space.

(2) vector patterns.
These are a variant of datatype OR nodes, where the OR-descrimination is on the
length of the vector pattern. The elements of the vector form an AND node.

A position in the pattern space is determined by a node in the AND-OR tree, which
can be described by a finite path down a branch from the root of the tree.  A
path can be defined as a finite list of "links", where links can be defined by
the datatypes

    datatype link
      = R of int      (* index into a record/tuple product pattern *)
      = VE of int     (* index into the elements of a vector pattern *)
      = D of datacon  (* choice among the decendents of a datatype OR node *)
      = VL of int     (* choice among vector patterns of different lengths *)
      = C of const    (* choice among constants, where const is the sum of the
                         transparent primitive types *)
    type path = link list

The root path is the empty list of links ([]: path).

- a node in an And-Or tree corresponds to a _position_ in the pattern space.

-- a node has an associated rule set that specifies the (rule) patterns
   that are consistent with that node

- a given target value can be propagated down through an And-Or tree, following
  each consistent branch and propagating in parallel though the children of an
  AND node. When it reaches a leaf node, the resulting rule set indicates which
  rule patterns are consistent with that value.  The possible matches are formed
  by taking the intersection of the leaf node rule sets.


== [8] Terms: AND-OR trees, choices, rule sets, relevance

An AND-OR tree is a representation of the pattern space generated by a
sequence of patterns. Each node represents a merging of the
subpatterns at a given point in the pattern space from each pattern in
the sequence. The original patterns all have a common type, and each
node of the AND-OR tree has a type derived from that common types.

It is possible to _match_ a value with an AND_OR tree by "pushing" the value 
(and its subcomponents)) down the branches of the AND_OR tree in parallel. Along
some branches the value will be eliminated because it is not consistent with a
choice (i.e. a nil value will be incompatible with a cons-labeled branch.

[9] A _choice_ (or _choice point_) is a point in the pattern space (a node in the AND-OR tree)
that discrimiates based on one of the following:

(1) datatype constructors (e.g. true vs false or nil vs cons)
(2) vector length
(3) constant value (int, word, char, or string)
    
Thus we distinguish three flavors of choice point. We don't distinguish between
different types of constants, (merging them under type constCon).

At each point in the pattern space (AND-OR node), there is a set of rules that are
compatible with that node (i.e. consistent with choices made on the path from
the root to that node). These rules are said to be "live", or "active" for that point.
Only the live rules actually have a subpattern at this point (as determined by a _path_).

[10] A choice is _relevant_ to a rule (ruleno), if that rule is:

(a) live for that choice point (hasn't been eliminated by an earlier choice along the
     path to this choice point), and 
(b) is not live in some immediate child of that choice point.


This is a binary relation between choices and rules.

Being relevant to more rules is considered a positive property of a choice.
It does more discrimination.

Building a decision tree is a process of choosing an ordering of choice points.

Choice points are inherently ordered by their position along branches in the AND-OR
tree. Thus a choice below a given choice must come after the higher choice (higher
and lower being determined by position along a branch. If two choices are not on the
same branch (i.e. the path to one does not go through the other), then they are 
inherently not ordered, or are _independent.

== [11] Variables at nodes

A variable can occur at a node in one of two ways:

(1) an atomic variable node (VAR)
(2) an "as"-bound variable attached to one (or more) of a nodes patterns

Atomic, or terminal variable nodes create defaults.  All rules that are live at
that node remain live through the entire subtree determined by that node (i.e. they
cannot be excluded by a choice.

For instance, consider the pattern sequence

(1)  cons(p1,p2)
(2)  nil
(3)  x

This forms an OR node at the root:

    []: OR(list) {1,2,3} var: (x,3)
         [cons]
	    AND {1,3}  [rule 2, nil, is eliminated, but not rule 3]
	      tree(p1)
	      tree(p2)
         [nil] {2,3} *

All the rules live at the top OR-data node remain live for the two immediate
children nodes.  Note that the variable x does not consitute a child node of
the OR-data node.  But it does influence propagation of liveness -- rule 3 remains
live throughout the subtree (e.g. at [D cons] and [D nil] and in tree(p1) and tree(p2)).

Compare this with

(1)  cons(p1,p2)
(2)  nil

with AND-OR tree:

    []: OR-data {1,2}
         [D cons]
	    AND {1}
	      tree(p1)
	      tree(p2)
         [D nil] {2} *

A layered variable binding does not extend the lifetime of rules:

(1) x as cons(p1,p2)
(2) nil

    []: OR(list) {1,2}
        as-var: (x, rule 1)
         [cons]
	    AND {1}
	      tree(p1)
	      tree(p2)
         [nil] {2} *

So a primitive var pattern creates a "default" rule that remains live below
(this) node, while a layered variable does not.


== [12] From AND-OR to Decision trees

(1) determine "accessible" list of CHOICE nodes
These are the CHOICE nodes that are accessible from a root (through selection
from AND nodes) without passing through another CHOICE node.  They are located
on paths which contain no OR links.


== [13] Constructing a Decision Tree

(1) Collect the list of "accessible" OR nodes in the AND-OR tree, with accurate
live rule sets, including default rules that result from VAR/VARS nodes.
The rule of a var binding remains live in the AND-OR node containing that variable,
and thoughout the subtree below that node (i.e. its rule cannot be killed by any
choice within that subtree, including the CHOICE that is the root of that subtree.
Note that VAR nodes may be merged with an AND node _above_ the CHOICE node that 
is being evaluated.

(1)  (x,nil)
(2)  (cons(p1,p2), nil)

Here the x at (RL 1) in the first rule creates a default for the choice node 
created by cons(p1,p2), so rule 1 will be live in the choice node generated from
the cons pattern.

As-bound variables (layer variables) do not have this defaulting effect, and therefore
don't affect the selection and ordering of choice nodes during the construction of the
decistion tree (?).  AS-bindings will have to be dealt with later during the translation
of the decision tree into match code.

Once a choice node is chosen as best from the initial list (the highest set of indepenent
choice nodes), we discard that choice from the list and enqueue the accessible choice
nodes from the subtree below the chosen node (if any).  So the set of remaining choice
nodes may grow (or shrink by one if there are no choice nodes below the chosen one in
the AND-OR tree.

[14] Rule sets associated with an AND-OR node:

(1) live rules: relation Live(rule,node)
Defn. A rule is live at a node if there is no OR choice above that node that is not
compatible with that rule. This means that there exists a value that could match that
rule. If the value was pushed down the branches of the andor tree, it would not have
been eliminated by the time it reaches this node.

(2) relevant rules (static?, or relevant to choice order in a decision tree?)
Defn: Relevance (static): a node is (absolutely) relevant to a rule if that
rule is live for the node (Live(rule,node)) and the rule is not live for some
immediate child of the node (i.e. one of the choices rules it out). Can a rule
ruled out for _all_ children?  Two cases

  (a) the rule introduces one of the children of the OR nodes (in which case it
      will be live for that child.
  (b) the rule does not introduce a new child
      (b1) it contains a variant that was already introduced by an earlier rule, in
           which case it is live for that variant
      (b2) it does not introduce a variant at all, in which case it must have
           a variable at this nodes position. In this case it becomes a default
           rule for this node, and for all its descendents, and is therefore 
	   considered "live" (= live union defaults)

(3) default rules: relation Default(rule,node)
Defn: a rule is a default rule at a node if there is a variable binding for that rule
at some node on the path to the node (including the node as the end of the path).
If Default(rule,node), then that rule cannot be ruled out at that node (i.e. the
node is not relevant to that rule.

Example:

  (1) cons(x,     nil)
  (2) cons(true,  cons(y,    nil))
[ (3) cons(false, cons(true, z))  ]
  (4) _

variable x in rule 1 makes rule 1 a default for the [R1,D(cons)] node. Rule 1 is
live for that node and any descendents of that node.

  [] OR(list) cons {1,2,3; 4} (vars = (_,4)) (partial)
     [cons] AND {1,2,3}
                [0] OR(bool) {1,2,3; 1,4}  vars=(x,1)
		      [true] LEAF {2; 1,4}
		      [false] LEAF {3; 1,4}
		[1] OR(list) {1,2,3; 4}
                      [nil] LEAF {1; 4}
		      [cons] AND {2,3; 4}
 		                 [0] OR(bool) {2,3; 2,4} (vars = (y,2)) (partial)
				       [true] # {3; 2,4}
				 [1] OR(list) {2,3; 4} (vars = (z,3) (partial)
				       [nil] {2; 34}

A _partial_ OR node is a datatype or node with not all datacons represented
in the children of the node.  

(All constant (except char) and vector OR nodes are partial by default.)

live(node) = {rule: Live(rule,node)}

live [] = allrules


================================================================================
[15] Theory: pattern space, paths, types
================================================================================

Defn. A patternable type is a type with some concrete product and sum (datatype)
structure at the top (where ty vector is an "honorary datatype descriminating on
the vector lenght).  Some primitive types (int, word, char, string) are also
deemed patternable.  Abstract types and function types are not patternable.

Defn. Abstractly, a tree is a prefix-closed set of paths (Milner, Webs, 1985). Each
path determines a "node" in the tree, and nodes can have attributes attached to them.
"Paths" are lists (finite or infinite) of "links", which may have structure and
attributes of their own.

Defn. Path concatention. If p and p' are paths, p@p' is the concatenation of the
paths, consisting of the links of p followed by the links of p' (the concatenation
of the paths as lists of links).

Defn. A path p' is a _prefix_ of a path p if E(p''). p = p'@p''. (depends on an
equality relation on links)

Defn. If T is a tree, T' \subset T is the _subtree_ at path p if T' consists of all paths
in T having p as an prefix.  Subtree(p,T) = {p' | p@p' \in T).  It is a tree (prefix closed).

Defn. An _initial_ tree of a tree T is a prefix-closed subset of T. A finite initial tree
is an initial tree that is finite.

For any patternable (ML) type t, there is a pattern tree P(t) that expresses the potential
pattern spaces of the type.  This tree has nodes of three kinds:

 (1) product (AND) nodes that may have a finite number of successors (or children)
     indexed by natural numbers (non-negative integers). The subtype at a product
     node is a product (record, tuple) type.

 (2) sum (OR) nodes, that have a finite number of successors indexed by "keys" that
     are either data constructors (datacons) for a datatype, or constants
     (for a patternable primitive type). The subtype at a sum node is a sum type
     (i.e. a datatype).
 (3) terminal (LEAF) nodes for constants (datatype or primitive)

Note: Vector types are treated as a kind of sum type (over some finite
range of vector lengths).  Natural numbers serve as keys for vector
types, indicating to the vector length.

Let t be a patternable type. A pattern pat: t is represented as an initial
tree of P(t).

Defn: Pat(p) \subset P(t) where pat: t.
(inductive definition on pattern structure or concrete structure of type t)

A variable in a pattern generates a (potentially infinite) subtree of the P(t).

Defn. The pattern space of a pattern list is the union of Pat(pi) for pi \in pat list.

This pattern space is represented concretely by an AND-OR tree.


================================================================================
[16] Variable patterns, live rulesets, default rulesets, propagation of defaults
================================================================================

Notational conventions for links, paths, keys:
  [n] represents [Rn], where n is the index of a product/tuple/record component (0 based)
  [K] represents [D(K,_)] where K is a dataconstructor name
  (not currently using other constant keys (num, char, string) in examples)

Conjectures:
1. live field of AND nodes do not play a role.  But live sets during decision tree
   building need to be propagated down through AND nodes.
2. Same for live field of VARS nodes?

[datatype t = A | B | C]
  (0) A
  (1) x
  (2) C

N0:
[] OR(t) {0,1,2}  Vars (x,2)
   [A] {0; 1x}
   [C] {2; 1x}

Rule 2 is default for the [A] and [C] variants.

Some rules may become "inaccessible" because of defaulting from variables.
In this example, rule (3) will never be "fired" in a match because it is
"shadowed" by rule (2).

Thus rule (3) will never be chosen on a branch of the decision tree (because,
while it may be live at a leaf, it will not be the _least_ live node at a leaf.
Here we have

  [] DEC(t)
     A {1,2x}  -- 1 chosen
     C {2x,3}  -- 2 chosen
     [B] {2x}  -- default branch

C leads to live set {2,3} which will select the least rule, i.e. (2).
B goes down an "else" branch with default ruleset {2}
A leads to live set {1,2} which selects rule (1)
Rule 3 is redundant, will never be matched.

r in Defaults(Node)
  ==>
  (1) Var(Pat(r)[Path(Node)]), or
  (2) Exists p < Path(Node). Var(Pat(r)[p])

which is the same as:

   Exists p <= Path(Node). Var(Pat(r)[p])

This means that Pat(r)[p] _cannot cause a mismatch_.

If for p0 < Path(Node), Var(Pat(r)[p]). r contributes no
pattern structure below p0. There may be pattern structure below
p, but it is contributed by other rules (earlier or later than r).

In above example: Var(Pat(2)[[]])


Defn: Relevant(N,r): An OR node N is relevant to a rule r if
  the choice made at that rule can affect whether that rule matches
  i.e. some variant is compatible with that rule and another variant
  is incompatible with that rule.
  i.e. Pat(r)[Path(N)] is not a variable, therefore is either
       a constant or a constructor (constant or applied)
       therefore Pat(r)[Path(N)] is a key for the choice made
       at N (N.variants).

Defn: Pat(r) (r a ruleno) is the pattern part of rule r (r.pat)

Defn: Pat(r)[p] = pattern element at longest possible prefix of
      path p in Pat(r).

Prop: Var(Pat(r)[N]) <=> r in Defaults(N)

Defn: Given an andor tree N for a given rule set
    Compat(r,p) if Pat(r) is "compatible" with all choices made
    on path p.

Notation: N an andor tree, p a path, Np is the andor tree found at
  the end of path p.
    N[] = N
    N[CL(c)] = LEAF ...
    N[DL(d)] = N' where N = OR{variants=ORdata [(d,N'),...],...}
    N[VL(l)] = N' where N = OR{variants=ORvec [(d,N'),...],...}
    N[RL(i)] = Ni where N = AND [..., Ni,...]
    otherwise, N

Defn: Compat(pattern, andor) : 
      Compat(pat, N[])  -- no choices made (yet) at root path
      Compat(c, N[CL(c)])   -- constant pattern; N is OR[ORconst] = LEAF
      Compat((p0,p1), AND(a0,a1)) if Compat(p0,a0) and Compat(p1,a1)
        -- and so on for n-ary products  (a0 = N[RL(0)], a1 = N[RL(1)]
      Compat(d, N[DL(d)])   -- constant datacon
      Compat(d(p), N[DL(d)]) if Compat(p,N)
      Compat(v, N))  true
   In which case r is live for node N(p)

pat@path = the subpattern (if any) of pat at the point designated
           by path (if "compatible")

Prop: Compat(r,p) <==> r in Live(Node(p)) ?

Prop: If Var(Pat(r)[path]), then r in Defaults(N(path)),
      where N = andor(rules).

Note: Pat(r)[path] is always defined, though the actual path for this
subpattern may be a strict prefix of path.


================================================================================
[17] Worked Examples
================================================================================

[18] Example 1:

datatype tree = L | N of tree * tree

      [0]         [N]
              [N0]  [N1]
  (0) N      (x,    L)
  (1) N      (L,    y)
  (2) z

AND-OR tree:
([n] is short for [Rn] (record selection n))

[] OR(tree) {0,1,2z}
   [N] AND {0,1,2z}
       [0] OR(tree) {0x,1,2z}
           [N] # {0x,2z}
	   [L] LF {0x,1,2z}
       [1] OR(tree) {0,1y,2z}
           [N] # {1y,2z}
	   [L] LF {0,1y,2z}
   [L] LF {2z}

Priorities:
  []   : (1,2)
  [N0] : (2,2)
  [N1] : (2,2)
  
  [] < [N0] = [N1]
  [N0] || [N1]  ([N0] and [N1] are compatible; diverge at AND node [N])
  [N0], [N1] dominated by [] ([] is a prefix of ...)

Decision Tree:

D[] {0,1,2(z)}
   N : {0,1,2z}
       Relevance:
         [N0] false (0 \in {0x,3z})
         [N1] true  (0 not \in {1y,2z})
     D[N1] {0x,1,2z}
        N*: {1y,2z}   ==> (1)
	    Relevance:
	      [N0] false (? not in {0x,2z}) [no _hard_ live rule]
	L : {0,1y,2z} ==> (0)
	    Relevance:
	      [N0] (0 in {0x,2z}) false
   L*: {2z} ==> (2)	       
       Relevance:  [N0], [N1] not compatible with [L] (diverge at OR[0])

Abbreviated Decision Tree:

  []
  N =>
    [N1]
    N* => (1)
    L => (0)
  L* => (2)

Note: OR node [N0] not used, because not relevant after [N1].

Code:

   Case vtop                 (vtop = v[0]: tree)
     of N v1 =>              (v1 = v[N]: tree * tree)
          letr (v2,v3) = v1  (v2 = v[N0]: tree, v3 = v[N1]: tree)
            in Case v3
	         of N => RHS 1
	          | L => RHS 0
        L => RHS 2

Code with external variable bindings:

   Case vtop                 (vtop = v[0]: tree)
     of N v1 =>              (v1 = v[N]: tree * tree)
          letr (v2,v3) = v1  (v2 = v[N0]: tree, v3 = v[N1]: tree)
            in Case v3
	         of N =>
		    Let1 y = v3 in RHS 1
	          | L => Let1 x = v2 in RHS 0
        L => Let1 z = vtop in RHS 2

================================================================================
[19] Example 2:

       [0]     [1]     [2]
 
  (0)  true,   false,  true
  (1)  true,   x,      false
  (2)  false,  true,   y

AND-OR tree:

[] AND
   [0] OR(bool) {0,1,2}
         [true]  LF {0,1}
         [false] LF {2}
   [1] OR(bool) {0,2; 3y}  var: (x,2)   [2 goes to defaults, not live]
         [true]  LF {2; 1x}
         [false] LF {0; 1x}
   [2] OR(bool) {0,1; 2y}  var: (y,3)
         [true]  LF {0; 2y}
         [false] LF {1; 2y}
   
Priorities:
   [0] : (0, 2)   (#defaults, width)
   [1] : (1, 2)   (#defaults, width)  (x, 2)
   [2] : (1, 2)   (#defaults, width)  (y, 3)

   [0] < [1] = [2]

Relevance: {1,2,3}, least: 1
   [0]: true  (1 /in {})
   [1]: true  (1 /in {2x})
   [2]: true  (1 /in {3y})

Decision tree:

D[0] {1,2,3}
   true  : {1,2}, least: 1
      Relevance: [1]: true  (1 not in defaults = {2})
                 [2]: true  (1 not in defaults = {3})
      D[1]  {1,3; 2x}
         true  : {2x,3} inter {1,2} = {2}, least: 2
	   Relevance: [2] : true  (2 not in defaults = {3}); [1],[2] compatible
	   D[2] {1,2,3y} > {1,3!}
	      true  : {1,3!} inter {2} = empty ==> MATCH!
	      false : {2,3!}, least: 2   
	        no more choices  ==> (2)
	 false : {1,2x} inter {1,2} = {1,2}, least: 1
	   Relevance: [2] : true  (1 not in defaults = {3}); [1],[2] compatible
	   D[2] {1,2,3y}
	      true  : {1,3!} inter {1,2!} = {1}, least : 1  ==> (1)
	        [[no more choices  ==> (1)]]
	      false  : {2,3!} inter {1,2} = {2}, least : 1  ==> (2)
	        [[no more choices  ==> (2)]]
   false : {3}
      Relevance: [1] : true  (3 not in defaults={2})
                 [2] : true  (3 not in defaults={2})
      D[1] {1,3; 2}
         true: {2!,3} inter {3} = {3}, least: 3  ==> (3)
	   [[ Relevance: [3]: false  (2 in defaults = {2})   [==> (3)] ]]
	 false: {1,2!} inter {3} = {},    ==> MATCH!
	   [[ Relevance: [3]: true  (1 not in defaults = {3})
	   D[2]: {1,2; 3}
	      true: {1,3!}, least: 1
	        no more choice nodes  ==> (1)
	      false: {2,3!}, least 2
	        no more choice nodes  ==> (2)  ]]

Abbreviated decision tree:

  [0]
  true =>
    [1]
    true  => 
      [2]
      true  => MATCH!   (counterexample: (true,true,true))
      false => (2)
    false =>
      [2]
      true  => (1)
      false => (2)
  false =>
    [1]
    true  => (3)
    false => MATCH!   (counterexample: (false,false,*))

Tests: (true,true,true) ==>  MATCH!
       (false,true,false) ==> (3)
       (false,false,false) ==> MATCH!

Code:
  Letr (v1,v2,v3) = root (argument/scrutinee)
    in Case v1
         of true =>
	     Case v2
	       of true  => 
		   Case0 v3
                      of true  => Raise Match
		       | false => RHS 2
		| false =>
                   Case0 v3
		      of true  => RHS 1
		       | false => RHS 2
          | false =>
	     Case v2
               of true  => RHS 3
	        | false => Raise Mmatch
		 

================================================================================
[20] Example 3:
(where a rule is chosen because no relevant OR nodes)

Type:    t * bool *  bool
        [0]  [1]     [2]
-----------------------------
  (0)  (A,   false,  true)
  (1)  (B,   x,      false)
  (2)  (z,   true,   y)

AND-OR tree

[] AND
   [0] OR(t) {0,1,2z}  -- [partial]
         [A] LF {0,2z}
         [B] LF {1,2z}
   [1] OR(bool) {0,1x,2}  var: 1x
         [true]  LF {1x,2}
         [false] LF {1,1x}
   [2] OR(bool) {0,1,2y}  var: 2y
         [true]  LF {0,2y}
         [false] LF {1,2y}
  
Priorities:
  [0] : (1,2)
  [1] : (1,2)
  [2] : (1,2)
(all three nodes have 1 variable, 2 keys ==> equal priorities)

top accessible = [[0],[1],[2]]

D[0] {0,1,2}  (= allrules)
   A  : {0,2z}, least: 0
      Relevance:
        [1]: true  (0 not in defaults = {1x}) -- chosen
        [2]: true  (0 not in defaults = {2y})
      D[1] {0,2}
         true  : {1x,2} inter {0,2} = {2}, least: 2  ==> (2)
	       Relevance:
	         [2]:  (2 in {2y}) false
	 false : {0,1x} inter {0,2z} = {0}, least: 0
	   Relevance: [2] : true  (0 not in defaults = {2y})
	   D[2] {0}
	      true  : {0,2y} inter {0} = {0}, least : 0  ==> (0)
	        [no more OR nodes  ==> (0)]
	      false : {1,2y} inter {0} = {}  ==> MATCH (A,false,false)
	        [no more choices -- irrelevant]
   B : survivors = {1,2z}, least: 1
      Relevance:
        [1] : false  (1 is in defaults[1]={1x})
        [2] : true  (1 not in defaults[2]={2y})
      D[2] {1,2z}
         true: {0,2y} inter {1,2z} = {2yz}
	   Relevance:
	     [1]: (2 not in {1x}?) true
	   D[1] {0,2y}
	      true  : survivors = {1,2z} inter {0,2y} = {2zy}
	         OR-nodes exhausted ==> (2)
	      false : survivors = {0,1x} inter {2} = {}  ==> MATCH
	 false: {1,2y} inter {1,2z} = {1,2yz}, least {1},  ==> (1)
	   Relevance: [1]: false  (1 in defaults = {1x})
	   no relevant OR node => choose least({1,2yz}) = (1)
   * : {2z} [not finished: e.g. (C,false,*) doesn't match]
      Relevance: [1] : (2 not in {1x}) true
                 [2] : (2 in 2y} false
      D[1] {2}
         true: {1x,2} inter {2} = {2}
           D[2] not relevant ==> (2)
	 false: {0,1x} inter {2} = {}  ==> MATCH (C,false, any)
	 
Abbreviated decision tree: (exhaustive)

  [0]
  A =>
    [1]
    true  => (2)
    false =>
      [2]
      true  => (0)
      false => MATCH  (A,false,false)
  B =>
    [2]
     true  => (2)
       [1]
       true => (2)
       false => MATCH  (B,false,true)
     false => (1)
  * =>  [C]
    [1]
     true => (2)
     false => MATCH  (C,false, *) 


=========================================================================================
[21] Example 4:
(where rule is chosen because no relevant OR nodes)

  (0)  A,  false, true
  (1)  B,  x,     false
  (2)  z,  true,  false

AND-OR tree

[] AND
   [0] OR(t) {0,1,2z}     var: 2z
           [A] LF {0,2z}
           [B] LF {1,2z}  (only _explicit_ keys appear in andor tree)
   [1] OR(bool) {0,1x,2}  var: 1x   [1 goes to defaults, not live]
           [true] LF {1x,2}
           [false] LF {0,1x}
   [2] OR(bool)] {0,1,2}
           [true] LF {0}
           [false] LF {1,2}

*** andor ***
[] AND {0,1,2} {}
   [0] OR {0,1} {2}  (vars: (z,3))
      A LEAF {0} {2}  {0,2z}
      B LEAF {1} {2}  {1,2z}
   [1] OR {0,2} {1}
      T LEAF {2} {1}  {1x,2}
      F LEAF {0} {1}  {0,1x}
   [2] OR {0,1,2} {}
      T LEAF {0} {}   {0}
      F LEAF {1,2} {} {1,2}

Priority:
  [0] : (1,3)  (2 variants + default = 3 branches)
  [1] : (1,2)  (2 variants, 2 constructors *)
  [2] : (0,2)

  [2] > [1] =? [0]

top accessible = [[0],[1],[2]]
all three OR nodes compatible: [0] || [1], [0] || [2], [1] || [2]
  (diverge from []AND )

Decision Tree:

D[2](bool) {1,2,3}  (= allrules)
  true : survivors' = {0}, least: 0
      Relevance:
        [0]: true  (0 not in {2z})
        [1]: true  (0 not in {1x})
        priority [1] > priority [0]
      D[1](bool) {0,1x,2}
        true : survivors' = {1x,2} inter {0} = {}  ==> MATCH 
	false: survivors' = {0,1x} inter {0} = {0}, least: 0
	   Relevance: [0]: (0 not in {1x}?) true
	   D[0] {0,1,2z}
	     A: survivors' = {0,2z} inter {0} = {0}  ==> (0) (no more OR nodes)
	     B: survivors' = {1,2z} inter {0} = {}  ==> MATCH
	     *: {2z} inter {0} = {}  ==> MATCH  <--- failure!!!  Catch this and generate MATCH
  false: survivors = {1,2}, least 1
      Relevance:
        [1]: (1 not in {1x}?) false,
        [0]: (1 not in {2z}?) true
           [1] not relevant, so [0] chosen, even though lower priority
      D[0](t) {0,1,2z}
        A: survivors' = {0,2z} inter {1,2} = {2}, least: 2
	   Relevance: [1] (2 not in {1x}?) true
	   D[1]: {0,1x,2}
	     true: survivors' = {1x,2} inter {2} = {2}
	       queue exhausted  ==> (2)
	     false: survivors' = {0,1x} inter {2} = {} ==> MATCH
        B: survivors = {1,2z} inter {1,2} = {1,2}; least 1
	   Relevance:
	     [1]: (1 not in {1x}? => false)
	     no more relevant tests  ==> (1) least of {1,2}
	*: survivors = {2z} inter {1,2} = {2z}
	   Relevance:
	     [1]: (2 not in {1x}?) true  (2 least live, even though from defaults)
	   D[1]: {0.1x.2}
	     true: survivors = {1x,2} inter {2z} = {2}  ==> (2) (OR node queue exhausted)
	     false: survivors = {0,1x} inter {2z} = {}  ==> MATCH
	     
Abbreviated decision tree:

  [2]
  true =>
    [1]
    true  => MATCH     (*,true,true)
    false =>
      [0]
      A  => (0)
      B  => MATCH      (B,false,true)
      * => MATCH       (C,false,true)
  false =>
     [0]
     A => 
       [1]
       true => (2)
       false => MATCH  (A,false,false)
     B => (1)
     * =>
       [1]
       true => (2)
       false => MATCH  (C,false,false)
  
Code:

  Letr (v1,v2,v3) = root
    Case v3
      true =>
        Case v2
	  true => MATCH
	  false =>
	    Case v1
	      A => RHS 2
	      B => MATCH
	      * => MATCH
     false =>
       case v1
         A =>
	   Case v2
	     true => RHS 2
	     false => MATCH
	 B => RHS 1
	 * =>
	   Case v2
	     true => RHS 2
	     false => MATCH
		

================================================================================
[22] Theory (Clarification): live ruleset, defaults ruleset, relevance, survival
================================================================================

(re: Construction of decision tree)

Rule sets are associated with andor nodes, which represent/designate points in the
pattern space (through their path).

Let's say the "live" ruleset is the set of all rules that have a chance of
  successfully matching at a given pattern point.

Live rules can be characterized as either "direct" or "default".

A rule is a "direct" live rule at p if that rule/p determines/contributes/matches
a particular discriminator key (constructor or constant or vector length). Thus

in the example 

       [p] OR(bool)
   (0) true   ...
   (1) x      ...
   (2) false  ...

rules (0) and (2) are directly live at [].  Rule (1) introduces a variable
default (1x), which also matches the direct keys true and false.  So for
this node:

  direct = {0,2}
  default = {1}

The live ruleset is

  live = direct U default = {0,1,2},

sometimes writen as {0,1x,2} to make it clear that 0 and 2 are direct and
2 is a default (introduced by the occurrence of x).

CONJECTURE: The relevance test should use the least rule in the _live_ set,
which includes both direct and default rules. This least rule may therefore
be a default rule.

Thus at a point in the decision tree construction, rule (0) may have been
eliminate by a higher ([]) false choice (in another column), in which case
the surviving live rules may be {1,2}.  A futher decision may be able to
elimate (1) even though 1 \in defaults.

At an OR node, there is a set of _live_ rules.  Each associated key for that
node determines a subset of live, live/key that is consistent with that key.
These are the rules that have that key at this pattern point, or are default
rules (because of having a variable at that pattern point or above on the path).

In the decision tree construction, the effective live set (for a given key)
is the intersection of live/key and the surviver set being passed down from
higher in the decision tree, i.e.

  survivors' = survivors inter (live/key)

This is the ruleset that should be passed to the call of makeDecisionTree
to compute that decVariant (key,dectree).  It eliminates direct rules with
keys other than the current key. E.g. in the above example, for key false,
live = {0,1x,2} and live/false = {1x,2} and if survivors = {0,2}, say, then
the new survivors for the call of makeDecisionTree for this decVariant is

  survivors' = {0,2} inter {1x,2} = {2}

CONJECTURE. direct and defaults rulesets are only active for OR nodes, so
they don't need to be computed or saved for other kinds of nodes: AND, SINGLE,
VARS, LEAF.

For an OR node, each ruleno in direct is associated with some key in
the variants. If we want to isolate the rulenos introduced with some
particular key, how can we do that?  Look at the andor in that key's
variant, use its direct ruleset. This will be a subset of the OR nodes
direct (?). But its andor may be an AND or a LEAF, or a SINGLE. So have
to be able to derive their direct and defaults sets. Thus any andor that
can be the andor of a variant will have to have direct and defaults.


================================================================================
[23] OR node queue management in DecisionTree
================================================================================
File: dec-tree.sml

OR nodes get "used up" while building a branch downward.
They do not get "used up" across separate, incompatible branches.

Hence, a node is passed down and as decision nodes are added, the queue
is eventually exhausted, _or_ the decision tree branches may terminate
while the queue is still not empty (redundant OR nodes).

Under each variant of an OR node, new (dominated) OR nodes may become
accessible, but OR nodes from "sibling" variants will be incompatible
and do not need to be added to the queue. Only OR nodes within that
variant andor need to be added for further consideration.

Defn: Two andor nodes or paths are compatible if the point where they first
diverge is not a choice key (i.e. is an R key).

Defn: An OR-node queue is (internally) compatible if any pair of nodes
in the queue are compatible.

Prop: If two nodes are incompatible, they cannot both be tested in a single
(dynamic) match.

Important invariants:

1. The OR node queue returned by the accessible function in OrderedOrNodes
will always be compatible, because all paths will differ at AND nodes --
OR-nodes terminate the traversal and their variants are not explored.

2. In makeDecsionTree (DecisionTree) The variantCandidates queue will be
compatible if the candidates queue returned by selectBestRelevant is
consistent, because all new OR nodes added lie beneath a single key variant
of the OR node chosen (from orNodes argument).
2.1. The top call of makeDecisionTree will be passed (as orNodes) the queue
produced by accessible applied to the top andor node.
2.2. Hence (inductively) all calls of makeDecsionTree will be passed compatible
queues.
2.3. selectBestRelevant will be passed a compatible queue, and the path
argument will be either (1) rootPath (for top call) or (2) the path of one of
the earlier, previously selected, compatible queues, and hence the path will
be compatible with all the nodes in the queue argument.  Hence the compatibility
test in the filter used in selectBestRelevant is redundant. (Check by
generating a warning message, or impossible message.)

================================================================================
[24] OR node priorities
================================================================================
The priority function is based on two factors:

  (1) number of default rules (fewer is better)
  (2) branching factor

At a first approximation, branching factor = number of variants
(length(variants)). But if there is a default rule (covered by a
variable) that can apply where there are missing constructors (keys),
then the branching factor can be

   length (variants) + 1  (for the default branch)

Does this occur only where variants are _partial_, meaning some keys
(e.g. datacons) are missing? [See example 4, path [0]].  Or is it
possible that a default with a _complete_ list of variants (covering
all constructors) can result in an increased branching factor?

Example (conjectural)

      p
  (0) T
  (1) F
  (2) x

Could the x in rule (2) result in a third branch at position p?

================================================================================
[25] Code generation
================================================================================
File: mc-code.sml

Provisional datatype for code: mcexp

Variables:
_Internal variables_ used to name value components. These are in 1-1 correspondence
with paths in the pattern space of the match (and hence AND-OR nodes). Could
generate a fresh internal variable as we construct each andor node, and then
would have the variables already available and "connected" with their path.

Otherwise, might maintain a mapping or two:

  variable --> path == node
  path/node --> variable

Multiple _external_ variables (the source variables that appear in the patterns)
may be associated (equivalent, denote the same value component) with a given
internal variable.  (i_variable <--> node => vars, asvars).  Will need parallel
bindings of internal and external variables (or bindings on external variables
_to_ internal variables).

Each internal variable will have a definite type == the type of the node/path,
== the type of corresponding value components.  Some of these types will be
represented by type variables (not metavariables/univariables!).
Make the abstraction over these "real" type variables explicit.

  AND(v0, [N1(v1), N2(v2), ..., Nn(vn)\])  ==>
   
    letr (v1,v2,...,vn) = v0  (* destructuring a product value *)
      (unpackaging(next choice))
       
  OR(v0, [(k1,N1(v1)), (k2,N2(v2)), ..., (kn,(Nn(vn)))])  ==>

    case v0          (v0 is the "scrutinee" *)
      k1 v1 => exp1  (= (unpackaging(next choice)))
      k2 v2 => exp2
      ...
      kn vn => expn

  For some keys, there will not be arguments, hence no variable binding needed.
  vi designates the ki-destructuring of the value bound to v0, etc.

  Example:

      (1) nil => rhs1
      (2) cons(x,y) => rhs2

      OR v0
        [(nil, LEAF({1})),
	 (cons, AND(v1, [VARS(v2,[(x,2)],{2}), VARS(v3,[(y,2)],{2})]))]

      case v0
        nil => rhs1
	cons v1 =>
	  letr (v2,v3) = v1
	    rhs2'     -- need to identify x with v2 and y with v3
	              -- or substitute (v2,v3) for (x,y) in rhs2

Observations
* Each OR node scrutinizes a particular point in the value structure
  (determined by its path in the pattern space), assuming the value
  is compatible with that path (agrees with OR choices along the path).

* Each value (component) scrutinized should be named by a (internal) variable.
  - A variable could be assigned to every AND-OR node, even though not all such
  variables might be needed -- but most of them will be used.  We don't need
  internal variables for LEAF nodes, since their partent OR node will have a
  variable naming the value already.
  - Some "internal" variables will pair with "external" variables (from vars
  or asvars fields).  How do we manage this association?
  

Example 3: [ datatype t = Leaf | Node of t * t ]

      Node   1               2      Node  1                  2
  (0) Node  (Leaf,           Node        (Leaf,              x))
  (1) Node  (Node(y, Leaf),  Node        (Node(Leaf, Leaf),  Leaf))
  (2) Node  (z,              Leaf)
  (3) Leaf

Andor:

1 [] OR(t) {1,2,3,4}
    [Node] AND {1,2,3}
2      [1] OR(t) {1,2,3z}     (var: (z,3))
            [Node] AND {2,3z}
4	      [1] OR(t) {2y,3z}  (var: (y,2))
5	      [2] OR(t) {2,3z}
	          [Node] - {3z}
	          [Leaf] # {2,3z}
	    [Leaf] # {1,3z}
3      [2] OR(t) {1,2,3}
            [Node] AND {1,2}
6	      [1] OR(t) {1,2}
	           [Node] AND {2}
8	             [1] OR(t) {2}
		          [Node] - {}
	                  [Leaf] # {2}
9		     [2] OR(t) {2}
		          [Node] - {}
	                  [Leaf] # {2}
                   [Leaf] # {1}
7	      [2] OR(t) {1x,2}  (var: (x,1))
	           [Node] - {1x}
	           [Leaf] # {1x,2}
	    [Leaf] # {3}
    [Leaf] # {4}

Priorities:
1 []                     = (0,2)
2 [Node,1]               = (1,2)   (z)
3 [Node,2]               = (0,2)
4 [Node,1,Node,1]        = (2,2)   (y,z)
5 [Node,1,Node,2]        = (1,2)
6 [Node,2,Node,1]        = (0,2)
7 [Node,2,Node,2]        = (1,2)   (x)
8 [Node,2,Node,1,Node,1] = (0,2)
9 [Node,2,Node,1,Node,2] = (0,2)

Decision tree:

  D[[]] {1,2,3,4}
    Node: {1,2,3}, least 1
       Relevance: [Node,1] true, (1 not in {3z})
       Relevance: [Node,2] true, (1 not in {})
       [Node,2] {1,2,3}  ("lower" priority)
         Node: {1,2} inter {1,2,3} = {1,2}
	 Leaf: {3} inter {1,2,3} = {3}
    Leaf: {4}, least 4  (do we need more choices in this case?)
       Relevance: [Node,1] true, (4 not in {3z}); Compatible: false
       Relevance: [Node,2] true, (4 not in {});   Compatible: false

* Only use "path compatible" tests.
  E.g. [Node,1] is not "compatible" with the decision branch [Leaf].
  so it should not be tested under Leaf.

* When choosing the next OR node in building a decTree, the set of
  OR nodes selected from should be "independent of"(?) and "consistent
  with" the current branch/node in the decTree that is being extended.

* Two nodes are consistent if their paths have no divergences.  A divergence
  is a point on the two paths where distinct and inconsistent keys are followed
  to the next node.  E.g. [... D(true) ...] and [... D(false) ...] where the
  true and false keys occur at the same point (the ith link) in the path.

* Being consistent means that both nodes can be "fired" during the matching
  of some potential value.

* In Example 3 above, the [Leaf] path is inconsistent with all the remaining
  OR nodes, since they all have paths beginning with [Node ...]. The first
  links (keys) in the paths are inconsistent. Therefore the [LEAF] node of
  the decTree is terminal -- there will be no further tests on this branch.
  
* Defn: Two paths p1 and p2 are incompatible ("diverge hard") if the first
  link (key) at which they differ is an OR (choice) key, e.g. D(true) vs
  D(false) or I(1) vs I(2).  Two OR nodes N1 and N2 are _incompatible_ if
  their paths Path(N1) and Path(N2) are incompatible.

  Prop: If OR nodes N1 and N2 are incompatible, then they will not both be
    tested when matching a value.  In particular, they will not be comparable
    in the path prefix ordering.  There is a test that dominates both N1 and
    N2 that discriminates between them, namely the test that is the source of
    the two divergent keys on their path.

  When selecting the next OR node while building the decision tree, the
  selected node should be both _relevant_ and _compatible_ with the "current"
  node.

Decision Tree 1 (8 ORs)

   []
   Node =>
      [N2]
      Node =>
         [N2N1]
         Node =>
            [N2N1N1]
            Node => MATCH!
            Leaf =>
               [N2N1N2]
               Node => MATCH!
               Leaf =>
 	          [N1]
                  Node =>
                     [N2N2]
	             Node => MATCH!
	             Leaf => (2)
                  Leaf => MATCH!
         Leaf => 
            [N1]
            Node => MATCH!
            Leaf => (1)
      Leaf => (3)
   Leaf => (4)
  
Decision Tree 2 (hand crafted) (8 ORs)

   [] 
   Node => {0,1,2}
      [N2]
      Node => {0,1}
         [N2N1]
         Node => {1}
            [N2N1N1]
	    Node => {} MATCH!
	    Leaf => {1}
	       [N2N1N2]
	       Node => {} MATCH!
	       Leaf => {1}
	          [N1]
		  Node => {1}
		     [N1N2]
		     Node => {} MATCH!
		     Leaf => {1} (1)
		  Leaf => MATCH!
         Leaf =>
            [N1]
	    Node => {} MATCH!
	    Leaf => {0} (0)
      Leaf => (2)
   Leaf => (3)

Code:
(No context around top decision node.)

  Case root  (* [] *)
    of Node v1 =>
         letr (v2,v3) = v1   (* v2 = Var[N1], v3 = Var[N2] *)
	   in Case v3  -- [N2]
	        of Node v4 =>
		     Letr (v5,v6) = v4
		       in Case v5  -- [N2N1]
		            of Node v7 =>
			         Letr (v8, v9) = v7
			     | Leaf =>
			         Case v2 ...
		 | Leaf => RHS 3
     | Leaf => RHS 4

--------------------------------------------------------------------------------
Example: mctest/t6.sml

fun f (x::y, z) = x > 0
  | f (nil, x) = not x;

andor:
[] AND v8 {0,1} {}
   [0] OR v9 {0,1} {}
      :: [0,::] AND v10 {0} {}
         [0,::,0] VARS v11 (x,0) {0}
         [0,::,1] VARS v12 (y,0) {0}
      nil LEAF {1} {}
   [1] VARS v13 (z,0) (x,1) {0,1}

dectree:
DEC [<0>]
  ::  DLEAF 0
  nil DLEAF 1

code: (for match only)

  letr (v9,v13) = v8  (= arg f)   ( v9 = x::y | nil ;  v13 = z | x )
    switch v9
       of :: v10 =>               ( v10 = (x,y) )
          letr (v11, v12) = v10   ( v11 = x_0; v12 = y_0; :: destruction )
	    v11 > 0               ( [v11/x_0] (x > 0) )
        | nil => not v13          ( [v13/x_1] (not x) )

For function f:

FN (v8) =>
  letr (v9,v13) = v8 (= arg f)     ( v9 = x::y | nil ;  v13 = z | x )
    switch v9
      of  :: v10 =>                ( v3 = (x,y) )
          letr (v11, v12) = v10    ( v4 = x(0); v5 = y(0) )
	    v11 > 0                ( [v4/x(0)] (x > 0) )
       |  nil => not v13           ( [v2/x(1)] (not x) )

Actual (buggy):

let v9 = #0 v8
  let v13 = #1 v8     (* no switch on v9: int list *)
    Int.> (v11,0)     (* no binding for v11 !!! *)

letr (v9, v13) = v8   (* top AND destruction *)
    v11 > 0           (* no switch, no binding for v11 !!! *)

FIX: return inner for VAR case of genTop in genNode in mc-code.sml.

--------------------------------------------------------------------------------
Example: mctest/t7.sml

fun f(x::y) = x;


================================================================================
[26] Adding types, polymorphism, type variable bindings
================================================================================

Observations:

1. The type of all the rules is known (post type checking).

2. The type of any andor node (path, point in the pattern space) can easily
be computed.

3. Be careful to distinguish between _real_ type variables and type
_metavariables_ (or unification variables), which are part of the type
inference machinery, but shouldn't appear in the types per se.
Definition of type needs to be modified to include _real_ type variables.

4. Real tyvars should have well-defined, explicit binding points.
Need a new Absyn construct for binding _real_ type variables.

Example: null

  fun null Nil => true
    | null (Cons(x,y)) => false
    
LHS pats
  (0) Nil           ConPat(Nil,tvs)  where tvs : tyvar list (produced by TC)
  (1) Cons(x,y)

[Typed] Code:

  let null = 
      TFN X =>   (* X is a real tyvar *)
        fn (x: X list) =>
	   Case[X list] x
	     Nil => true[bool]
             Cons v => false[bool]   [v : X * X list, not used]

  null : (All X) X list -> bool

Where do we get X?  From an (uninstantiated) tyvar extracted from where?
From the pattern(s) type or from tvs stored with datacons (which are lists
of type metavariables).

tvs : tyvar list left by TC, which may be instantiated, in which case
their instantiations may contain further uninstantiated ty metavariables
which should be abstracted.

The pattern type could either be stored by the type checker in the
"rule" datastructure, or reconstructed from the basic "constructor"
values in the patterns (or just first pattern?) and the instantiation
information in the tvs fields for datacons and the element type for
vector patterns.  Uninstantiated metavariables in the pattern type
could then be instantiated to new "real" tyvars, which could be abstracted
over by a new TFN form in Absyn.

-------
Probably best to add types to the variables as they are created during
makeAndor.  The top type would be passed to makeAndor along with the patterns.
It would be broken down into appropriate components as the function recurses
over pattern structure (which is coordinated with the type structure anyway).
The top type should have "generalized" type variables instantiated to real
tyvars, but it may contain tyvars generalized in an outer scope, so the list
of tyvars generalized "at" this match should be available explicitly, and
the match code would be wrapped in a TFN binding of these "locally" generalized
tyvars.  The type passed to makeAndor should be compressed and free of
meta-tyvars, which makes it simpler to break down (no pruning needed).

The type checker needs to be modified to:

(1) intantiate the generalized meta-tyvars to fresh _real_ tyvars, and

(2) add these abstracted/generalized tyvars to the abstract syntax in
some appropriate way (perhaps as a temporary kludge, there might be a
tyvar list ref element included in the appropriate construct (match, fn, binding, ?).
When type checker is modified to produce a new "typed" abstract syntax, the
representation could be more direct in terms of an appropriatley scoped binding
construct like TFN.

The inferred+generalized type of the lhs of the rules (i.e. the patterns) should
also be available in the absyn in order to be passed to makeAndor with the
patterns. [It could be reconstructed by effectively re-typechecking the patterns,
using the embedded types of dcons and variables if we want to avoid changing
the absyn util the type checker overhaul.]

** Explicit polymorphic abstraction (TFN) in the match code

Derived from the polymorphic type of the match.
Abstraction over "real" typevars, which in turn have been used to "instantiate"
the polytype. This instantiated polytype is what is used as the type argument
to makeAndor.

(also bindings, handler matches need to be treated appropriately)


Note: FNexp, CASEexp, HANDLEexp are all based on a match, represented as

  fnrules = AS.rule list * ty * ty

Here the components are:

  rules : AS.rule list
    -- the actual list of rules (no extra default rule added), each
       rule consisting of a pattern (lhs) and an expression (rhs)
  lhsTy : T.ty 
    -- the common type of the patterns, the "domain" or input type
       of the match

  rhsTy : T.ty
    -- the common type of the rhs expressions, the "range" or output type
       of the match

The lhsTy is needed for match compilation as, for instance, the initial
type argument of makeAndor, which establishes the pattern type and
by deconstruction the types of all the pattern nodes and their associated
(s)vars.

The rhsTy is needed for a rather paticular purpose, to provide the
type to assign to artificial (generated) raise expressions during
match compilation (in particular, passed as an argument to the Failure
function in vmcexp.sml). This rhsTy was just added to the definition
of fnrules in Abysn.  When match compilation (MatchComp.matchComp) is
invoked, this type is passed as an argument and is passed on to
MCCode.code and then to VMCexp.Failure.

[For source occurrences of "raise", the _return_ type, which is
arbitrary, is determined by returning a fresh metatyvar that will
"soak up" the context type through unification. This can't be done for
the occurrences of "raise" _manufactured_ by the match compiler, hence
the need for an explicit passing of the rhsTy.]


================================================================================
[27] External (source) variable binding and RHSs linkage and dispatch
================================================================================

After the decision tree is constructed, we need the following:

For each _occurrence_ of a source variable in a pattern, we need
(1) the rule number of that occurrence (available in vars fields), and
(2) the corresponding internal varialbe (with its type) for that node in the pattern
  space (andor tree)

Then at each LEAF node of the decision tree, we need (during code generation) to add
bindings of the source variables of the chosen rule to the appropriate internal
variables.

We also need to have a count of how many times a rule occurs at a LEAF node of the
decision tree.
  -- if it occurs 0 times, the rule is redundant
  -- if it occurs > 1 times, we need to form a RHS function by abstracting over
     the rule source variables, and at each of its LEAF nodes, we need to
     apply that RHS function to the tuple of corresponding internal variables.
     
Create a mapping from
  (source var, ruleno) to (internal) svar (equivalently the andor path)
  ruleno to set of (source var, path)

What we have in the andor tree is essentially
  path -> (source var, ruleno)  (through the vars and asvars fields)

Another even more useful mapping would be:

  varmap: ruleno -> [(source var, svar), ...]

Could this mapping be constructed (on the side) while makeAndor builds the
andor tree?  Only issue is where a VAR node is constructed first and then another
structured pattern is merged into it, perhaps replacing the svar of the VAR node
with a new one [as in the next to last rule of mergeAndor]. This could be fixed
by _resetting_ the svar field of the node produced by the call of mergeAndor in this
rule.

Otherwise, could do a global post-pass on the entire andor tree to construct this
mapping.

   for each node N:
     for each (v,r) in N.vars: insert(r, (v, N.svar), varmap)   (* destructively? *) 
     for each (v,r) in N.asvars: insert(r, (v, N.svar), varmap)
(this could also be done functionally)

Then when constructing the linkage for a RHS (LEAF) node for rule r, we lookup
r in var map: (v_ext,v_int) then do let-bindings let v_ext = v_int in ...
around the rhs expression.

Or, if the rhs is shared by multiple rules, form

   f = (lambda (v_ext1,...,v_extn) . rhs_exp)

at the dispatch point

   rhs_r = f(v_int1,...,v_intn)

the bindings of the dispatch function f would be wrapped around the body of
the match expression.

NOTE: Each rule introduces its own _local_ bindings of its own pattern variables.
A given variable _name_ may occur in different rules (even with different types!),
but these variables will be unique to each rule.  For example:

fun f (x::y, z) = x + 3 > 0
  | f (nil, x) = not x

Here x in the first rule is of type int, while x in the second rule is a different
locally bound variable of type bool.  So the two "rule variables" (<x>, 0) and
(<x>, 1) involve two different variables, both named x, with types int and bool,
respectively.  The two variables named x will have different lvars (access values).

NOTE: Variables appearing in patterns are _not_ polymorphic (they are
lambda-bound in fun matches), and their occurrences in patterns are not assigned
instances of a polymorphic type. Therefore (?)  the tyvars field of a pattern
VALvar should contain the empty list. The type of a pattern variable may contain
metatyvars introduced by instantiating polymorphic types of datacons.  Those
metatyvars may be "generalized" at the level of a val/fun binding containing
the match that contains the pattern, but they can be treated (after typechecking)
as fixed during the analysis and translation of the match.

================================================================================
[28] "Code generation" (mc-code.sml) Generating the absyn for a match
================================================================================
[Preliminary notes, extracted from mc-code.sml]
----------------------------------------------------------------------------
When we are generating code for a decTree D (at node N), the surrounding
structure for D has been "destructed" to provide a context that, in particular
has a binding for Var(Node(D)).  During dynamic matching, Var(D) will be bound to
the value component being matched to decTree.
 
When an AND node is destructed, we get a Letr binding of all the variables (lvars) of
the component nodes of the AND.  We need to remember that these have been bound
(and are still in scope?) when we need to use one of those variables for some
decision. Their scope os the body of the Letr exp.

So at each subexpression we can keep track of which variables are in scope
at that subexpression (a set of lvars).  Veriables for the nodes of variants
are bound in branches of Case0 expressions.  These are in scope only in the
exp of the corresponding switch branch.

How much of the top of the pattern space has been destructured before we
deal with the first decTree node (OR node), D?  I must be at least enough to
bind Var(Node(D)).  At the top level, the whole value being matched is bound
to vtop.

(1) find root andor node of dtree and construct code to access that node
(2) generate Case0 for root of dtree
    (2a) for each decVariant for that root, "find" arg component corresponding
         to the associated variant decTree (meaning construct access code).

 * Given a decTree node (and associated andor node and its path), need two bind a variable
   to the correspoinding value component (using a nesting of letv bindings).
 * Among the existing variable bindings, which is closest along a path from vtop (the
   root variable bound to the entire argument value) to the path to the target..

 * given a path (of the next decTree node), find the nearest (lowest) variable
   bound along that path and construct access to bind a variable to the given path.

   Example:  decTree D @ p0 where p0 = [k1, k2, k3, k4, k5, k6]
    The case0 code for D must be placed in a mcexp context that "unravels" the
    path p0 and binds a variable v_p0 to the arg value at p0 (val_p0).
    Maybe this is represented as a kind of "continuation", or "context" expression?

    Suppose a variable v is bound at k3 (has path [k1, k2, k3]) and it is the "closest"
    variable on the path, i.e. there are no variables bound at k4, k5, k6.
    Suppose k4, k5, k6 are R1.D(true).R2   (k4 = R1, k5 = D(true), k6 = R2)
    The D(true) key on this path means there has been a previous decTree at the
    path p1 = [k1, ..., k4], with true as one of its keys. The OR node at p1 "dominates"
    the OR node underlying the decTree at  

    letv (v1,v2) = v (the variable at k3)
       Case0 v1
         true =>   (bind a variable here? No, because of nullary key true.)
   
    If 

Example:
AND(a1,a2,a3)
  
let vtop = arg
letv (v1,v2,v3) = vtop
    (v1: R1, v2: R2, v3: R3)

get dtree
dtree.node.path = R1  --> v3 = vtop.R1

    Case0(v1, branches)
    variants = R1.variants = (k1, a11) :: (k2, a12) :: arest
       branch1 = (k1, dt1) :: dtrest1
       k1 => code(a11,dt1)
       branch2 = (k2, dt2):: dtrest2  (where dtrest1 = (k2,dt2)::dtrest2)
       k2 => code(a12,dt2)


what if dt_root (root node of decTree) has path R1.R3?

   letv (v1,...) = vtop
    letv (w1,w2,w3) = v1   (w3 = vtop.R1.R3)
      case0 w3
       etc. ....

auxiliary info

   path -> variable (for already bound variables)
   variable -> path

(for "visible top-and structure")

for each variable binding in the code, can record path for the variable.
 
given a path, can produce a path relative to some existing bound variable

CONJECTURE: if N1 is an ancestor of N2, then Var(N1) is in scope at the
"(code) position" of N2.


================================================================================
[29] Vector patterns
================================================================================
The vector key or con case discriminates on vector length, but that discrimination
does not destructure the vector (i.e. does not "strip" a constructor). Instead,
the vector is left intact and must be destructed explicitly by a nested sequence
of Vector.sub (vector subscript) operations.

* There is no absyn form for vector subscripting (also no syntax form for record
  selection. We could add such forms, or we can create some other kind of
  syntactic representative for record selection and vector subscripting (what
  would that be?).

* When you "virtually" strip the V (VLEN) constructor when discriminating on
  vector length, the resulting "stripped" value is the same vector -- it is not
  turned into a record/tuple. Therefore the children of a vector OR node are
  vectors, and should be represented by (new) vector andor nodes (VEC). When
  the corresponding vector values are deconstructed, it will be done by a
  series of nested lets involving vector subscripting:

    letv (v0, v1, v2) = (sv: ty vector) in body
    ==>
    let v0 = Vector.sub(sv,0) in
      let v1 = Vector.sub(sv,1) in
	let v2 = Vector.sub(sv,2) in body

  (expanded by a "wrapLets" function).

  How do we represent

  (1)    let v1 = Vector.sub(v0, i) in body

  in Absyn?

  For records, we could possibly expand <<select>> into the
  appropriate expansion of #n, i.e. (fn (_,_,_,x,_,_) => x) for
  tuples, but this would not work for vectors, and it would
  reintroduce a (simple) record pattern match that would have to be
  tranlated.

  We could introduce an Absyn for for the primop Vector.sub,
  and another one for record selection. Say

  datatype exp =
    ...
    | selVec of exp * int  (* n >= 0 *)
    | selRec of exp * int  (* n >= 0 *)

  Then the absyn for (1) would be

    let v1 = selVec(v0,i)

  The types would be

    selRec : {f0: t0, ..., fn: tn, ...} * int -> tn

    selVec : ty vector * int -> ty

  The first of these is anomalous, but we don't need to deal with these
  operators during type checking because they do not occur in the surface
  abstract syntax.  Also, during translation, the index of selection/subscripting
  will be known statically, so the type (of a record selection) will be known.

  Alternatively, selRec and selVec could be a pair of "handmade" constants
  (VALvars) that could be inserted in the abstract syntax during match
  compilation, so that 

    val selVec = VALvar{name = <selVec>, ...}
    selVec(v0,i)  ==>  APP(VARexp (selVec,_), NUMexp i)

  and similarly for selRec.

  A related approach would be to add destRec and destVec as new variants of
  VALvar, whose typing and translation would be treated in an ad hoc manner.
  This has the advantage of not needing to construct an artificial and inaccurate
  type for the VALvar form. But the application of these special "variables"
  would have to be detected and handled specially during translation.

* NOTE: This analysis indicates that _converting_ the contents of a vector
  into a tuple (AND node) during match compilation is incorrect. We need to
  preserve the distinction between records/tuples and vectors through match
  compilation and translation.  Therefore we need to add a new VEC node to
  the andor type.

* NOTE: first approach (7/14/20) chosen is to add SelRec and SelVec as
  new variants of VarCon.VALvar.  These will need to be handled
  specially in translate.


================================================================================
[30] Variable bindings (VB); irrefutable patterns; single, irrefutable rules
================================================================================

Defn: a pattern is irrefutable if it will successfully match any value of
 its type.

* An irrefutable pattern is made up of products (records/tuples), variables,
and singleton datacons (with or without arguments): 

Defn: Pattern p is _irrefutable) iff p is constructed by these three constructions:
    (1) p == (p0, ..., pn)  where pi is irrefutable; or
    (2) p == v, a variable (VarCon.var)
    (3) p == Dcon {p}, where p (if present) is irrefutable

Note: These three constructions correspond to the AndOr node constructors
handled by genNode in mc-code.sml.

Defn: a pattern is refutable if it is not irrefutable

* A refutable pattern has a refutable element, which could be

   (1) a constant (int, word, char, string); or
   (2) a nonsingular datacon (e.g. true, false, nil, cons, ...)
       (a datacon belonging to a datatype with more than one datacon); or
   (3) a vector element (AND[VECTOR]) (vector patterns are always refutable)

The refutable elements of (a list of) patterns are the elements that give
rise to OR-nodes in the AndOr tree for the patterns.

An irrefutable pattern has an AndOr tree with no OR nodes (only VAR, AND[RECORD],
and SINGLE nodes).  Note that any vector element gives rise to an OR-node
(of vector type), with variants leading to AND[RECORD] nodes.

If an irrefutable pattern occurs in a list of rules, any rules following the
one with the irrefutable pattern are redundant (unreachable).  The rule with
an irrefutable pattern serves as a default rule (if the earlier patterns are
not exhaustive).

Example:  pats = [(x,y)]

  (0) (x,y)   =>  rhs0     

AndOr: andor(pats)

  [] AND ('X * 'Y)
     [0] VAR (x, 0)
     [1] VAR (y, 0)

Decision Tree:

  DLEAF 0  (trivial, no OR nodes)

Code:

  Letr (sv_x, sv_y) = svroot in rhs0

  == genNode (andor(pats), genDec (DLEAF 0))
     == genNode (andor(pats), rhs0)

Lexp:

  LET lvar_x = SELECT (0, lvar_root) in
    LET lvar_y = SELECT (1, lvar_root) in trans(rhs0)


In the event of there being no OR-nodes, decisionTree will produce the result
DLEAF 0, which genDec will translate to rhs0. genNode (applied to the pattern
AndOr tree, will produce the necessary patten destruction code to wrap around
rhs0.


================================================================================
[31] Translating VB declarations (e.g. val (x,y) = e, or val x::y = e *)
================================================================================

NOTE: We don't need to worry about the "top-level" case -- that should be handled
by the code at the end of the main function transDec in Translate. So we only
need to properly handle the "local" case, which has a _body_ expression.  But
we probably need to get the management of "boundtyvs" (for vbs) and "btvs"
(for VALvars) right so that they will be translated into type abstractions and
applications.

Naive translation:

Example 1: irrefutable pattern, no case

  val (x,y) = def

local decl:

  let val (x,y) = def in body

  ==>

  let r = def in
    let x = SELECT(0,r) in
      let y = SELECT(1,r) in
        body

top-level decl:

  - val (x,y) = def;

  ==>

  local
    let r = def            (* VB [vb_r] *)
  in
    let x = SELECT(0,r)    (* VB [vb_x, vb_y] *)
    let y = SELECT(1,r)
  end

Example 2: refutable pattern

  val x::y = def

local decl:

  let val x::y = def in body

  ==>

  case def
    of x::y => body
     | _ => raise Bind

  let v0 = CASEexp (def, [(cons(vx,vy), (vx,vy)], isBind)   (* isBind = false *)
    in let x = select(v0, 0) in
         let y = select(v0, 1) in
	   body

  [WRONG!]
  let v0 = def in   (* v0 a fresh var = rootvar returned by matchComp *)
    switch v0
      [(cons(sv1), let x = select(sv1,0) in let y = select(sv1,1) in body)]
      SOME (raise Bind)
      

top-level decl: (We don't have to deal with this because Translate.transDec
  takes care of it. All we need is the generic transVB and transVBs functioins.)

  - val x::y = def;

  ==>

  local
    let r = def    (* VB [vb_r] *)
  in
    val (x,y) = 
      case r
        of ::(u,v) => (u,v)
	{{ | nil => raise Bind }}  (* one rule, bindExn default *)
  end

  local
    val r = def  (* r a fresh var *)
    val d =      (* cons destruct of r; d a fresh var *)
      CASEexp(VARexp(ref r, tvs???),
              [(cons(x,y), (x,y))]
	      bindExn)
      -->
      switch r
        [(cons(v), VAR v)]
	SOME (raise Bind)   (* may raise Bind *)
  in
    val x = select(d,0)
    val y = select(d,1)
  end


* What about a declaration that is part of a sequence of declarations?

Example 3: (local) 

  let
    val a = 3       (* VB [vb_a, vb_xy, vb_p] *)
    val x::y = e
    val p = e'
  in body

we could preprocess this into a nested sequence of lets:

  let val a = 3 in
    let val x::y = e in
      let p = e' in
        body

Then the second let would be expanded as above.

Similarly for 1st decl in local?

--------------------------------------------------------------------------------
Here is the pseudo-code for transVB:

transVB( VB{pat, exp, boundtvs, ...}, body) >>
  1. If pat is a simple variable pattern v (VARpat, with or without type constraint),
     then "Let v = exp in body" (with something to deal with polymorphism of exp?).
  2. For general, compound pat:
     a. patvars = variables in pat (in some order, e.g. left to right)
     b. patvarstuple = and expression creating a tuple of the patvars values
     c. matchComp ([RULE(pat, patvarstuple)], ...):
        ==> (matchExp, rootvar) 
        This match will destruct the exp (value) to build the tuple of pattern
	variable values.  
     d. Let rootvar = exp in matchexp  ==> tuple of var values
        This causes the exp value to be destructed by matchexp, yielding the
	tuple of pattern variable values.
     e. wrapLets(patvars, 0, body)
        wraps the body in a sequence of let bindindings that bind the original
	pattern variables with the corresponding components of patvarstuple
     f. Bind the variable tuple to a new var, and execute e:
        let newvar = (let rootvar = exp in patvarstuple) in wrapLets(patvars, 0, body)

Missing details?  How to preserve and propagate information about polymorphic
types of the pattern variables, which may be instantiated where they occur in body.
-- Is the existing tyvar annotation of the variables enough?
-- What is mkPE doing? Do we need a corresponding transPolyExp?
-- This problem may be handled more clearly and systematically once we have
   a fully explicit typed absyn. (after type-checker rewrite)

CLAIM: We don't need to treat irrefutable bindings specially, since
the special case will be produced automatically by the *code* function
(i.e., one rule, no default, destructing and variable binding).

Should we try to unify the treatment of local decls and top-level
decls by applying the same decl -> decl translation to both?  Or
does the tranlation need to distinguish the local and top-level
cases?


NOTE: It may be possible to "restore" use of source variables (in
mc-code.sml) by mapping (some) SV.svars back to V.vars using the
ruleVarMap for a given rule (a bijection between source variables in
the rule pattern and a subset of internal svar variables.  At some
point, this could be used to "map back" from svars to corresponding
vars.  This could at least partially eliminate the need for Letm.  The
trick is to figure out the right place to do this.


================================================================================
[32] RHS linkage; direct or via rhs functions
================================================================================

For direct linkage, for single-use rules, we use Letm to match up
the V.vars that appear in the rhs expression with the corresponding
svars (which were collected from the ruleVarMap (MCcode.code; mc-code.sml)).

We leave the vars occurences in the rhs expression alone (instead of
"alpha converting" them to the corresponding svars.  We can't go back
and retrofit the vars into the andor/decisiontree/mcexp, since a given
svar (representing a pattern point) will in general be shared among
several rules (and possibly be "equivalenced" with several different
source vars in those rules - or even with multiple source vars in a
single rule, because of patterns like "x as y").  So we can't unify
the vars and svars by replacing svars by vars (and of course some
svars don't have equivalent vars anyway).

QUESTION: What if we, during match code generation (MCCode.code), agressivly
replaced svar binding (and applied) occurrences in the generated code by
matching vars when available -- but this would only make sense in the
context of a particular rule, since the var <-> svar mapping is per rule.
Is the rule known and well-defined when we are making the svar bindings?

================================================================================
[33] TransMatch: translating matches in an Absyn to Absyn transformation
================================================================================
The "compilation" or translation of matches (case, handle, binding flavors) needs
type information, or at least is most natural to perform in the presence of types.
This means that we can't do it before type checking, in the basic elaboration
phase. It also doesn't work to tie match compilation in with type checking, since
with the interleaving of typing and match compilation it is impossible (or at least
difficult, to guarantee that the desired type information is produced before needed
in match compilation.  So the cleanest approach is to do match compilation in
a separate post-type-checking phase that we call transmatch (structure TransMatch).
With this approach, the input is fully typed absyn (though it would be more
convenient if it were fully _explicitly_ typed).

This translation preserves the absyn except for the small number of constructs
where matches are to be "compiled".  The case and handle constructs are fairly
straightforward because there is no tricky interaction with polymorphism.  The
valbind case, where polymorphism is produced, is trickier (see transVB). The
issue is producing absyn that the translation phase (translation to Plambda.lexp)
can deal with properly.

================================================================================
[34] Polymorphism: representing and maintaining it when translating val bindings
================================================================================
The relevant functions in the old translator are mkVB (translating vbs) and
mkPE (translate an expression with potential type parameters, i.e. a polymorphic
expression on the rhs of a binding).

The main problem seems to be to coordinate between the polymorphism arity (btvs)
of a variable (a pattern variable in the decl) defined as a selection from a
tuple bound to another polymorphic variable of a different arity (boundtvs).
Can we get away with just considering the btvs lists of the two variables, perhaps
using the assumption that the btvs metatyvars are a subset of the boundtvs
metatyvars, which is actually the case in the translation of a VB.  Is this
a coincidence that won't work in general? (But maybe it doesn't need to work
in general?)

OBSERVATION: The btvs of a pattern variable is a subset of the boundtvs of
the entire pattern.  The btvs's of the pattern variables may overlap, but
the union of their btvs will be equal to the pattern boundtvs stored in the
VB record.

For a particar pattern variable getting a polymorphich type, the polymorphic
abstraction should be done over the btvs of the VALvar, which is a reference
set by TypeCheck.generalizeTy.  A variable bound to the pattern variable tuple
would be polymorphic over the union of btvs of the pattern variables, which
should be the same as the VB boundtvs.

The rebindings of the pattern variables themselves (transVB.wrapLets) should
re-establish the polymorphism of those variables when translated.

NOTE: for the call of MC.matchComp in transVB, the VB record must be modified
by adding a typ field (to be defined by the type checker). This type will be
the common type of the pattern and the rhs expression. [This type could be
reconstructed, essentially by calling the type checker again on the pattern,
but that was done already -- why not just store the result for later use?]

There are two flavors of VB:

(1) _administrative_ VB used in _administrative_ lets, say within the
expressions produced by match compilation for case and handle constructs.

(2) _polymorphic_ VB associated with valbind declarations.

These may be (partly) differentiated by the fact that administrative
valbinds are not meant to introduce polymorphism, and therefore will always
have boundtvs = nil.  Other non-administrative valbinds may also have
nil boundtvs, simply because there is no polymorphism in the binding
(e.g. val x = 3).

So the question is how the polymorphic nature of some valbinds is
preserved by match compilation. We need to judiciously use VB with
non nil boundtvs. Generally, these will be simple VBs, with the binding
pattern being a single variable. The btvs of that variable will indicate
the degree of polymorphism (arity of TFN).  But in applied occurrences,
those variables will then need type arguments (TAPP args).  How do
we compute these type arguments.  An example is displayed in the old
Translate mkVB function (inside of mkVBs).

Is there a general approach to calculating TFNs and the corresponding
TAPPs, or is it a set of ad hoc situations like the selections produced
by mapLets in transVB?  Hopefully this will be clearer when we have
fully explicit typing of absyn, where these type abstractions and applications
will be spelled out in the abstract syntax.

OBSERVATION: After match compilation (transMatch), all LETexp valbinds
have a single variable pattern.  A subset of these (from source VBs) will
involve polymorphism, but the polymorphic variable will be unique, since
compound patterns have been compiled away (by transVB).


================================================================================
[35] OR patterns
================================================================================
As a first approximation we interatively merge the two patterns:

[src]

      | mergeAndor (ORpat(pat1,pat2), ty, rule, rpath, andor) =
	mergeAndor(pat1, ty, rule, rpath,
		   mergeAndor(pat2, ty, rule, rpath, andor))

This doesn't work because, for example, in the following test case (t16.sml)

[src]
    datatype t = A of int * int | B of int * int;

    fun f (A(x,y) | B(x,y)) = x + y;

the pattern-bound variables x and y get duplicated, and the duplicates have
different associated svars that they should bind to.  There will be a single
rhs function for this rule (rule 0 in this example), but it should be applied
two different pares of svars in the two branches of the code (branching on
A/B) where it is called.  The absyn produced by match compiling should look
something like:

    let val rhsfun0 [lv31] =
	      (fn SfunArg [lv32] =>
		    let val y [lv20] = #0 SfunArg [lv32] in
		      let val x [lv21] = #1 SfunArg [lv32] in
			Int.+ (x [lv21],y [lv20])  (* rhs *)
		    end end)
    in SWITCH <> [lv24]
	  of B <B> [lv25] =>   (* B branch with svar <B>, lvar 25 *)
	       let val <B.0> [lv26] = #0 <B> [lv25] in  (* y svar *)
		 let val <B.1> [lv27] = #1 <B> [lv25] in (* x svar *)
		   rhsfun0 (<B.1> [lv27], <B.0> [lv26])
	       end end
	   | A <A> [lv28] =>   (* A branch with svar <A>, lvar 28 *)
	       let val <A.0> [lv29] = #0 <A> [lv28] in  (* y svar *)
		 let val <A.1> [lv30] = #1 <A> [lv28] in  (* x svar *)
		   rhsfun0 (<A.1> [lv30], <A.0> [lv29])
	       end end
    end

[BUG: Right now, 2020-8-18, 4 arguments are being passed to rhsfun0, two copies
of x (from A and B branches) and two copies of y (from A and B branches), but
only two of those argument variables are bound in each of the two calls.]

When generating the branches of this switch, how do we know which versions
of x and y should be bound and used in each branch?

We assume (Elabcore.elabPat) that a single version of each pattern variable is
used in both the patterns in an OR pat.  This is, in the above example, the
x in A(x,y) is the same (or equivalent) VALvar as the x in B(x,y), with the same
lvar. ** Check that this property is ensured by elabPat. **

Currently we reconstruct the mapping from source vars to administrative svars
for each rule using a separate pass over the andor tree (RuleVarMap.makeRuleMap).

We could capture this mapping while constructing the andor tree. I.e. for any
VARpat pattern we could register the mapping from its var to the svar for this
node.  But in the case of the OR pattern above, we would be registering x and
y twice, with different svars.  How to we distinguish between these svar
bindings in MCCode.genRHS?  Note that the rule number is not useful for
distinguishing these cases since there is only one rule number involved
(e.g. 0 in this case).

IDEA: associate a path with each var -> svar binding, and at a terminal genRHS
at path P, pass only those var bindings that are "compatible" with P. This should
filter out bindings belonging to another branch of the decision tree.

A binding therefore is logically a tuple or relation involving 4 components:

   (1) rule -- the rule the variable occurs in
   (2) var  -- the source variable (var)
   (3) path -- the path that determines the pattern node where the variable occurs
   (4) svar -- the svar (internal adiministrative variable) that var should bind to

Because of OR patterns, a source variable x may have multiple occurrences in
the pattern of a given rule. For a given RHS invocation, there should be only
one version (occurrence) of x passed to the RHS function.

For a given rule (and hence RHS), we will have a mapping var -> svar * path,
and we pass only those svars where the path is compatible with the RHS path.

This mapping could be constructed like the ruleVarMapping by a post-traversal
of the andor tree, or it could be constructed during the construction of the
andor tree, but that may be more complicated.

CHANGES: 
(1) In RuleVarMap (matchcomp/varmap.sml), add path to the (var,svar) list
for each rule. So rulemaps are ruleno -> (var * path * svar) list.

Example [35.1]:

  datatype t = A of t * t | B of t * t | L of int;

  fun f ( A( ( A(L x, L y) | B(L x, L y) ), L 2 ) | B (L y, L x) ) = x + y;

3 versions of the variable pair (x,y), at "decision" paths A.A, A.B, B.
(1) x @ <A.0.A.0.L>, y @ <A.0.A.1.L>
(2) x @ <A.0.B.0.L>, y @ <A.0.B.1.L>
(3) x @ <B.1.L>, y @ <B.0.L>
[all at rule 0]

andor:
[<>] OR v26 {0} {}
   B [<B>] AND v27 {0} {}
      [<B.0>] OR v28 {0} {}
         L [<B.0.L>] VARS v29 (y,0) {0}
      [<B.1>] OR v30 {0} {}
         L [<B.1.L>] VARS v31 (x,0) {0}
   A [<A>] AND v32 {0} {}
      [<A.0>] OR v33 {0} {}
         B [<A.0.B>] AND v34 {0} {}
            [<A.0.B.0>] OR v35 {0} {}
               L [<A.0.B.0.L>] VARS v36 (x,0) {0}
            [<A.0.B.1>] OR v37 {0} {}
               L [<A.0.B.1.L>] VARS v38 (y,0) {0}
         A [<A.0.A>] AND v39 {0} {}
            [<A.0.A.0>] OR v40 {0} {}
               L [<A.0.A.0.L>] VARS v41 (x,0) {0}
            [<A.0.A.1>] OR v42 {0} {}
               L [<A.0.A.1.L>] VARS v43 (y,0) {0}
      [<A.1>] OR v44 {0} {}
         L [<A.1.L>] OR v45 {0} {}
            I2 LEAF {0} {}

dectree:
CHOICE [<>]
  B CHOICE [<B.0>]
    L CHOICE [<B.1>]
      L DLEAF 0  <B>, <B.0.L>, <B.1.L>   -- dtrace for DLEAF
                 <B.0.L> ~ x, <B.1.L> ~ y      longest common prefix : <B>
                 <A.0.A.0.L> ~ x : lcp: <>
      * MATCH    <B>, <B.0.L> <B.1.~L>    [ B(L *, ~L) ]
    * MATCH      <B>, <B.0.~L>            [ B(~L, *) ]
  A CHOICE [<A.1>]
    L CHOICE [<A.1.L>]
      I2 CHOICE [<A.0>]
        B CHOICE [<A.0.B.0>]
          L CHOICE [<A.0.B.1>]
            L DLEAF 0  <A>, <A.1.L> <A.0.B>, <A.0.B.0.L> <A.0.B.1.L>   -- dtrace for DLEAF
	               <A.0.B.0.L> ~ x,  <A.0.B.1.L> ~ y   longest common prefix: <A.0.B>
                       <B.1.L> ~ x  longest common prefix: <>
		       <A.0.A.0.L> ~ x  longest common prefix  <A.0>
            * MATCH (~L)
          * MATCH (~L)
        A CHOICE [<A.0.A.0>]
          L CHOICE [<A.0.A.1>]
            L DLEAF 0  <A>, <A.1.L>, <A.0.A.0.L>, <A.0.A.1.L>   -- dtrace for DLEAF
	               <A.0.A.0.L> ~ x,  <A.0.A.1.L> ~ y   longest common prefix: <A.0.A>
            * MATCH (~L) [<A.0.A.1.~L>]
          * MATCH (~L) [<A.0.A.0.~L>]
        * MATCH (~A,B -> L)  <A.0.B.0.L>
      * MATCH (~I2)  [<A.1.L.~I2>]  [ A(*, L(~I2)) ]
    * MATCH (~L)  [<A.1.~L>]  [ A(*, (~L) *) ]
  * MATCH (~A,B -> L)  [<L>]  [ L * ]

>>>makeRuleMap
I: y[22] ==> <B.0.L>[29]
I: x[23] ==> <B.1.L>[31]     -- B
I: x[23] ==> <A.0.B.0.L>[36]
I: y[22] ==> <A.0.B.1.L>[38] -- A.0.B  = lcp (<A.0.B.0.L> and <A.0.B.1.L>)
I: x[23] ==> <A.0.A.0.L>[41]  
I: y[22] ==> <A.0.A.1.L>[43] -- A.0.A
<<<makeRuleMap

let val rhsfun0 [lv46] =
          (fn SfunArg [lv47] =>
                let val y [lv22] = #0 SfunArg [lv47]
             	 in let val x [lv23] = #1 SfunArg [lv47]
              	     in let val y [lv22] = #2 SfunArg [lv47]
                         in let val x [lv23] = #3 SfunArg [lv47]
                             in let val x [lv23] = #4 SfunArg [lv47]
                                 in let val y [lv22] = #5 SfunArg [lv47]
                                     in Int.+ (x [lv23],y [lv22])
                end end end end end end)
in (SWITCH <> [lv26]
      of B <B> [lv27] =>
           let val <B.0> [lv28] = #0 <B> [lv27]
            in let val <B.1> [lv30] = #1 <B> [lv27]
                in (SWITCH <B.0> [lv28]
		      of L <B.0.L> [lv29] =>
		           (SWITCH <B.1> [lv30]
			      of L <B.1.L> [lv31] =>
			         rhsfun0  (* too many args *)
				    (<A.0.A.1.L> [lv43], <A.0.A.0.L> [lv41],
				     <A.0.B.1.L> [lv38], <A.0.B.0.L> [lv36],
				     <B.1.L> [lv31], <B.0.L> [lv29])
                               | _ => raise Match)
                         | _ => raise Match)
           end end
       | A <A> [lv32] =>
         let val <A.0> [lv33] = #0 <A> [lv32]
          in let val <A.1> [lv44] = #1 <A> [lv32]
              in (SWITCH <A.1> [lv44]
                    of L <A.1.L> [lv45] =>
                         (SWITCH <A.1.L> [lv45]
			    of I2 =>   (* int const 2 *)
                               (SWITCH <A.0> [lv33]
                                  of B <A.0.B> [lv34] =>
				     let val <A.0.B.0> [lv35] = #0 <A.0.B> [lv34]
				      in let val <A.0.B.1> [lv37] = #1 <A.0.B> [lv34]
				          in (SWITCH <A.0.B.0> [lv35]
					        of L <A.0.B.0.L> [lv36] =>
						     (SWITCH <A.0.B.1> [lv37]
						        of L <A.0.B.1.L> [lv38] =>
							     rhsfun0 (<<the 6 variables>>)
                                                         | _ => raise Match)
                                                 | _ => raise Match)
                                         end 
                                     end
				   | A <A.0.A> [lv39] =>
                                     let val <A.0.A.0> [lv40] = #0 <A.0.A> [lv39]
                                      in let val <A.0.A.1> [lv42] = #1 <A.0.A> [lv39]
                                          in (SWITCH <A.0.A.0> [lv40]
                                                of L <A.0.A.0.L> [lv41] =>
                                                     (SWITCH <A.0.A.1> [lv42]
                                                        of L <A.0.A.1.L> [lv43] =>
                                                             rhsfun0 (<<the six variables>>)
                                                         | _ => raise Match)
                                                 | _ => raise Match)
                                         end
                                     end
                                   | _ => raise Match)
                             | _ => raise Match)
                     | _ => raise Match)
             end
         end
       | _ => raise Match)
end

Algorithm:
At a RHS node for rule n (DLEAF n). Get the decision tree branch for this DLEAF
decision tree node, say [p0, ..., pk] (the paths at decision points on the dt branch).

Given a set of variable-path pairs (v,p), and the decision tree branch
[p0, ..., pk], calculate the longest common prefix of p with
p0, ..., pk, call it lcp.  Choose all variables v such that lcp is maximal for binding

Claim 1: each chosen v is "in scope" for the DLEAF (RHS dispatch).

Claim 2: Any "in scope" variable occurrence (v, p) will have the maximal length lcp
with the branch.
-----------------------

This attempt didn't work (or wasn't chosen). Instead, instrument decision trees
with "traces", which are lists of paths, where the path designates the choice
points (including the choice) along a maximal branch of the decision tree, i.e.
a branch terminating in DMATCH or DLEAF.  At a DLEAF terminal node where we
dispatch to a RHS, the trace to that node is used to determine which source
variables (vars) are "consistent" with the choices in the trace, and only
consistent variables are "bound" for that RHS.  See MCUtil.consistentPath for
definition of "consistency" used.


= [35.1] Legal OR-pattern requirements

OR-patterns like (v | v) or ((v,_) | (_,v)) are a problem because there is no
_discrimination_ involved. A possible general principle for excluding such cases
is:

  OR-Principle:  p1 | p2 is legal if there are values that match one but not the other,
  i.e. diff([[p1]], [[p2]]) and diff([[p2]], [[p1]]) are both nonempty, where [[p]] is
  the set of values matching p.  We can use the notation p1 # p2 for this situation,
  i.e.,

  Defn: p1 # p2 iff not ([[p1]] \sub [[p2]]) and not ([[p1]] \sub [[p2]]).

  p1 # p2 => there must be some common pattern point where p1 and p2 are incompatible,
  i.e. Exists path. p1[path] # p2[path] where
  p[path] # q[path] if key(p[path]) <> key(q[path]).
  [This formulation is not quite right since the key is the last element in the path?
   Correct this defn.]
  [Try this: Exists path1 in p1 and path2 in p2 s.t. path1 # path2 (they clash at some
   OR key (not a projection key).]

So [[p1]] == [[p2]] is not allowed, nor [[p1]] \sub [[p2]] (or visa versa). There have
to be values that match p1 and not p2, and visa versa.

Note that

  p1 | p2 => ...

is not going to be equivalent to either

  p1 => ...
  p2 => ...

or

  p2 => ...
  p1 => ...

--------------------------
** Reppy/Successor ML**
In p1 | p2, p1 has "priority" -- it matches first.
Under this rule,

      (v,_) | (_,v)  -->  (v,_)

because the first pattern, "(v,_)" has _priority_.  The ambiguity is resolved on the
basis of "left to right" matching.
An objection is that it was not necessary to use the OR pattern in this case, and using
it presumably indicates some misconception or confusion on the part of the programmer.
Another objection is that it injects a left-to-right ordering or priority into the
process of matching a single pattern, which is antithetical to the freedom used by a
match compiler.
--------------------------

It is still possible for OR-patterns to have common "top-level" structure (i.e.
discrimination points may be embedded below), as in

  ([],v) | (v,[])

Here some values, e.g. ([],[]) match both patterns (in which case the first
pattern has "priority" -- or does it matter?), but there are values that are
discriminated: ([],[1]) matches p1 but not p2, and ([1],[]) matches p2 but not p1.

If [[p2]] \sub [[p1]] (p2 is subsumed by p1), for instance, then p2 is redundant
and p1 | p2 could be simplified to p1.

If [[p1]] == [[p2]], then p1 | p2 is redundant and could be replaced by either
p1 or p2.

What about the situation where [[p1]] \inter [[p2]] is non-empty?

Defn: p1 < p2 if [[p1]] \sub [[p2]], i.e. every value matching p1 also matches
p2. A slight variation on a naive pattern matching algorithm can compute this
relation, which is a partial order.  Clearly:

Prop. p1 < p2 ==> (p1 | p2) == p2.

So p1 | p2 could be reduced to p2.

Prop. not (p1 < p2) and not (p2 < p1)  =>  symetric difference of p1 and p2 <> emptyset.

I.e. Exists x \in p1 - p2 & Exists y \in p2 - p1.
[[p1]] - [[p2]] <> \empty & [[p2]] - [[p1]] <> \empty.

But [[p1]] .sd. [[p2]] = {} does not imply that p1 and p2 are _independent_.

Prop: For any pattern p, [[p]] <> {}. I.e. any pattern is (self) _consistent_.

Prop: [[p1]] \inter [[p2]] = {} => p1 and p2 are independent.

Defn: p1 and p2 are inconsistent if [[p1]] \inter [[p2]] = {}, i.e. not value
matches both patterns.  E.g. p1 = true, p2 = false; p1 = nil, p2 = r::s.

Prop: p1 and p2 inconsistent => p1 and p2 independent.

Prop: p1 and p2 independent Exist paths r1 in p1 and r2 in p2 s.t. r1 # r2.

Defn: For paths r1, r2, r1 # r2 if there are initial segments q.k1 of r1 and q.k2 or r2
where k1 = k2 and k1 and k2 are not R keys (product projections).

Algorithms for testing

(1) Inclusion: p1 < p2  (All v) match(v,p1) => match(v,p2)
    View p1 as a pseudo-value and match it with p2.

(2) Independence: p1 # p2 if not(p1 < p2) and not(p2 < p1).
    This defines independence in terms of inclusion, but there is probably a
    more direct (and efficient) algorithm.

Rule: (p1 | p2) is legal iff p1 # p2.

Prop: If p1 and p2 are independent, then [[p1 | p2]] = [[p1]] \union [[p2]].

----------------------------------------------------------------------
Theory II
---------

Proposed principle: The | (OR) operator on patterns should be commutative (and
associative).

If the semantics of OR is expressed solely in terms of the matching set of values,
then this would be obvious and natural:

   [[p1]] | [[p2]]  =  [[p1]] \union [[p2]] = [[p2]] \union [[p1]] = [[p2]] | [[p1]]

But matching patterns also produces variable bindings (sets of variable, value pairs).
Thus a pattern like

     (x,_) | (_,x)

is ambiguous in terns of the variable binding produced:

    ((x,_) | (_,x)) $$ (2,3)

could produce either x->2 or x->3, depending on the order of the matching.
A sufficient condition for p1 | p2 to be unambiguous in terms of variable matching
is that [[p1]] and [[p2]] be disjoint (no values match both p1 and p2).  If neither p1
nor p2 contain variables, then p1 | p2 is unambiguous even if they are not disjoint,
e.g.  ((_,_),_) | (_,(_,_)), but these, (at first glance), do not appear to be useful,
so it is not clear whether it is worth supporting this exception to disjointness.

Implementation Note: Does disjointness need to be tested before pattern analysis
(e.g. construction of an And-Or tree), or can the disjointness test be integrated
into the normal analysis?

For p1 | p2 to be legal, p1 and p2 must be disjoint (unambigous) and env(p1) = env(p2):
i.e., p1 and p2 must bind the same variables at the same types.

If p1 and p2 are both variable free, the env(p1) = env(p2) = \empty, and p1 | p2
would be well-defined even if p1 and p2 are not disjoint.

Additional pattern operators:

1. sequential matching

   p1 ; p2

Matching this compound pattern matches p1 first, and if successful returns its
corresponding environment. If p1 $$ v fails, then it attemps to do p2 $$ v.  p1 and
p2 must bind the same variables at the same type: env(p1) = env(p2).

2. pattern negation

   !p

A value matches !p if it fails to match p.  No environment is returned, and p should
not contain variables (other than wildcard): VF(p).

How to implement p1;p2 and !p needs to be investigated.

Either of these could be used to express Andreas's example

   (C x, _) | (_, x)

as

(1)  (C x, _) ; (_, x)

or

(2)  (C x, _) | (!(C _), x)

Defn: vars: pattern -> variable set
      vars(p) = set of (free) variables occurring in p, disregaring "_" (wildcard)

Defn: tenv : pattern -> (variable f-> type)  (f-> == finite mapping)
      tenv(p) = mapping from free variables in p to their types
      	        as determined by type checking

Defn: Venv = finite maps from vars to values
      venv : patterns * values -> Venv
      venv(p,v) = finite mapping from vars(p) to their matching values, assuming v \in [[p]].

Defn: VF(p) is true if p contains no variables (not counting wildcards).

Defn: type(p) = the type of p (as determined by type checking)

Defn: Full(p) iff [[p]] = [[type(p)]] (p matches every value of its type).

Note: [[p1]] \sub [[p2]] does not imply that (All v) venv(p1,v) = venv(p2,v).

Reductions: 

Examples:



================================================================================
[36] Unique bound lvars and lvar alpha-conversion
================================================================================
The fcontract optimization phase (FLINT/opt/fcontract.sml) appears to require
the property that lvar bindings are unique, i.e. that the same lvar is not bound
more that once (even in separate nonoverlapping scopes).  t21.sml is an example
that fails because it does not have this property.

To ensure that lvars are unique, it seems that the whole algorithm involving
the interaction of matchComp and transMatch needs to be done "top-down" rather
than "bottom-up" (as it is initially done). This is so an lvar environment can
be passed down from an lvar binding point to the "match subexpressions" that may
use that lvar (or var/svar having that lvar as access). This environment can
map "original" lvars arrising from the svars of the AND-OR tree to new
"alpha-converting" lvars.

A bottom up approach that tries to alpha-convert the exp produced by
transMatch+matchComp would be very expensive, since lower level subexpressions
would have to be alpha-converted repeatedly as higher-level lvar bindings
are discovered.

The top-down approach means that transMatch and matchComp need to be
mutually recursive, passing an lvar env down through the recursive calls.

** SOME INVARIANTS **

* At a DLEAF node, DLEAF (r, trace), the set of variables available in the
varenvMC at that node is exactly the set of variables in the pattern for r.
This is also the domain of the varenvMC available at this node.

* Each of these variables has exactly one svar for this rule in the
current varenvMC (and the corresponding path is consistent with the trace).


================================================================================
[37] OR pattern validation/normalization
================================================================================

[Terminology Note: OR patterns are no longer analagous to OR nodes in the AndOr
tree. They should probably be called something like "layered patterns".]

Here are some examples of "funny" OR patterns:

(1)   OR(x, x)     "x | x"
(2)   OR((x,true), (x, y))   "(x,true) | (x, y)"
(3)   OR(x, 1::x)  "x | 1::x"

== OR Normalization: a normalized OR pattern OR(p1,p2) is one where p1 and p2 are
incompatible at their roots.

We can reduce OR patterns to either (1) a non-OR pattern, or (2) a normalized
OR pattern:

(i)    OR(x,x)  ==>  x    -- the OR is eliminated

(ii)   OR((p1,p2), (q1,q2)) ==> (OR(p1,q1), OR(p2,q2)) (for tuples and records)

So (2) normalizes to (x, OR(true, y)).

(iii)  OR(p, x) ==> x;   OR(x, p) ==> x   -- the OR is eliminated

For (iii), consider

   OR(x, 2::x)

This is ambiguous (at least partly).  The nil value would match only the
first pattern x, while [2,3] would match both sides.  For [2,3], the
resulting binding of x could be either x -> [2,3] or x -> [3]. A reasonably way
of resolving the ambiguity is to have the variable x dominate 2::x and reduce
or normalize the OR pattern to x.  Similarly for OR(2::x, x).

Goal: an OR pattern should translate to an OR node in the AND-OR tree.  The
two patterns should therefore split/conflict at their top nodes. Or, in other
words, the two patterns p1 and p2 should generate an OR-node andor tree.  We can
push OR down through product patterns and eliminate it if one of the patterns
is a variable.

Example:

      OR((x,true), (x, y))   "(x,true) | (x, y)"
      ==> (OR(x,x), OR(true,y))
      ==> (x, y)


--------------------------------------
37.1 OR pattern checking or validation
--------------------------------------

Rather than "normalizing" OR patterns to remove undesired examples, we can establish
criteria for "legal" OR patterns and generate error message when an illegal OR pattern
is detected.  Here is a proposal for the desired properties that a legal OR pattern
must satisfy:

An OR pattern OR(p1,p2) is legal if:

(1) p1 and p2 have the same type ty, which is also the type of the OR pattern.

(2) p1 and p2 must have the same set of bound pattern variables, i.e.,
FV(p1) = FV(p2).

(3) p1 and p2 must be _disjoint_, where

Definition: Patterns p1 and p2 are _disjoint_ if no value matches p1 and p2, i.e.
Val(p1) \intersect Val(p2) = \emptyset, where V(p) is the set of values matching p.

Algorithmically, we can test whether p1 and p2 are disjoint by

(1) p1 and p2 are product patterns and there exists a field f such that p1.f and p2.f
are disjoint.

(2) p1 and p2 are sum patterns whose top tags (keys, constructors) are different.
E.g. 1 | 2;  true | false;  nil | p::q

(3) if p1 or p2 is a variable, then they are not disjoint

Conjectured proposition:

Defn: Pattern conjunction. The conjunction of patterns p1 and p2, denoted p1 & p2,
is a pattern which is matched by values matching both p1 and p2.  Thus
Val(p1 & p2) = Val(p1) & Val(p2).

Prop. If p1 and p2 are not disjoint, then there exists a pattern p such that
Val(p) = Val(p1) \intersect Val(p2) = Val (p1 & p2). ("meet" in pattern space).

Actually, this proposition is probably not true.  Counterexample?

Also there is the relation p1 < p2, such that Val(p1) \subset Val(p2) -- any value
matching p2 also matches p1.  A variable is a maximal pattern by that ordering.
(p1 and p2 assumed to be of the same type).

Question: Find two patterns p1 and p2 such that p1 and p2 are not disjoint, but
neither p1 < p2 nor p2 < p1 holds.

Do patterns form a lattice under <?  NO. (not with variable bindings considered?)

Defn: Univ (p) if [[p]] = [[type(p)]]; matches all values of its type.

Defn: Null (p) if [[p]] = {}; matches no values (of its type).

Pattern Variables
-----------------
Two simple patterns are _variable-congruent_ if they have the same variables bound
at the same pattern paths.

It is not clear what the corresponding notion is for OR (cor or por) patterns.


-----------------------------------------
37.2 Semantics of a pattern (2nd version)
-----------------------------------------

[[p]] : values -> (vars f-> values)

where vars is the set of variables in the pattern.  The domain of this map is the
set of values matching p.  For each value v in the domain, ([[p]] v) is the map
from variables (in p) to the value component that matches at that variable's path.
(unique since each variable has a unique occurrence in the pattern). Thus ([[p]] v)
is a value environment.

With this definition of semantics, we can rephrase the definition of variable congruence
as

Defn: p and q are variable congruent if [[p]] = [[q]].

Thus [[p]] and [[q]] have the same domain and for every matching v, ([[p]] v) = ([[q]] v)
as environments. [this is with respect to semantic equality, not computational equality.]

This definition could work for OR patterns as well, since the semantics of OR patterns
can be expressed in the same way: for a pattern p and a value v that matches p, the
match produces a unique value environment.

Conditions for commutative OR patterns:

Possible conditions for p | q == q | p when p and q are simple:
1. p and q are disjoint: {p} /\ {q} = {}
2. (All x ini vars p), path(p,x) = path(q,x).  (note:  vars(p) = vars(q))

What changes if p and q contain or are OR patterns?

OR Pattern simplification reductions:

1a. x | p => x   (but not p | x => p, based in l2r priority)

1b. _ | p => _   (vars(p) = {})
1c. p | _ => _   (vars(p) = {})

2. (p1,p2) | (q1,q2) => (p1 | q1, p2 | q2)  (OR distributes over AND), if ... (vars)?)

3. C p | C q => C (p | q)  (distributes through data constructors)

4. (p1 | p2) | p3 => p1 | (p2 | p3)  (associativity "to right")

5. (p1 | (p2 | p3)) => (OR(p1,p2,p3))  (associativity, flattenin to n-ary OR)

6a. (x as p) | (x as q) => x as (p | q)  (same variable x)
6b. (x as p) | q => x as (p | q) ?

E.g.

   (true, _) | (_, false) => ((true | _), (_ | false)) => _ | _ => _

   ((x, y) | (y, x)) => (x|y, y|x) => (x,y)

Counter example to 2:

   (x, y, z) | (z, y, x)  !=> ((x | z), (y | y), (z | x))

   because (x | z) and (z | x) are ill-formed OR patterns -- the two
   patterns don't contain the same variables.

Is there some variant of reduction rule 2 that is valid?

7. Horizontal vs vertical overlays (layers):
  Horizontal:

     true | _     (r, s.0) vs (r, s.1)

  vs vertical:

     true => e    (0)
     _    => e    (1)
     p    => e'   (2)

In the vertical case, true has priority over _ because it is assoicated
with rule 0, verses _ associated with rule 1.  The same should be the case
for the horizontal overlay. The _ at rule 1 will "shadow" (default) patterns
for any subsequent rules.

8.  true | _  =>  (0, .0) vs (0, .1)
    false     =>  (1)

Here the _ in rule 0 (0, .1) shadows the false in rule 1.  So a false value will
match at rule 0.

9. datatype d = A of int | B of int | C of int

   (a) A x | (B x | C x) => RHS(r)   (ruleno r)

   (b) y as (A x | (B x | C x)) => RHS(r)

   (c) (y as A x) | ((y as B x) | (y as C x)) => RHS(r)

   (d) (y as A x) | ((y as B x) | y) => RHS(r)

   (e) A x  => RHS(0)
       B x | C x => RHS(1)

Suppose the layer at the path where these patterns occur is (r.s) (r a ruleno, s
an orpath (i.e. bitstring).

  (a) OR (d) [@ r.s]
        A :
	  VAR [vars: x @ r.s.0]
	B :
	  VAR [vars: x @ r.s.1.0]
	C :
	  VAR [vars: x @ r.s.1.1]

  (b) OR (d) [@ r.s] (asvars: y @ r.s)
        A :
	  VAR [vars: x @ r.s.0]
	B :
	  VAR [vars: x @ r.s.1.0]
	C :
	  VAR [vars: x @ r.s.1.1]

  (c) OR (d) [@ r.s]
        A :
	  VAR [vars: x @ r.s.0; asvars: y @ r.s.0]
	B :
	  VAR [vars: x @ r.s.1.0; asvars: y @ r.s.1.0]
	C :
	  VAR [vars: x @ r.s.1.1; asvars: y @ r.s.1.1]

  (d) Not legal, since Vars(y as B x) = {x,y} while Vars(y) = {y}.
      Should fail the check for equal sets of pattern varialbes (having same types).

  (e) OR (d) [@ 0, 1]
        A:
	  VAR [vars: x @ 0]  => RHS(0)
	B:
	  VAR [vars: x @ 1.0] => RHS(1)
	C:
	  VAR [vars: x @ 1.1] => RHS(1)

      Note: RHS(1) has a multiplicty of 2, so needs to be abstracted.


--------------------------------------
37.3 Note on pattern/layer subsumption
--------------------------------------

Prop: [[p]] < [[q]] => [[p | q]] == [[p]].

The implimentation must satisfy this proposition. (precise defn of < needed).


-----------------------------------
37.4 Note on variable "consistency"
-----------------------------------

    true | _

is ok, Vars = {} on both sides.

    true | x

is illegal, since Vars{true} = {} and Vars(x) = {x}, not equal.

    (x as true) | x

is ok again: Vars(x as true) = {x} = Vars(x).

This is symmetric, as

    _ | true

and

    x | (x as true)

are legal, but degnerate:

  [[_ | true]] = [[_]] = \v: bool.empty-env.
  [[x | x as true]] = [[x]] = \v: bool.{(x,v)}
  

-------------------------------------------
37.5 Note on pattern/path/rule relationship
-------------------------------------------

Assume a sequence (list) of rules, with the corresponding list
of patterns, indexed by rule numbers: pats = [p0, ..., pn] = #1 rules.

Defn: If r is a ruleno, then pat(r) is the pattern (LHS) of the
rule numbered r. These patterns are also the verticle layers of the match.

Defn: Let PS(m) be the pattern space of a match m (a list of rules).
If path p \in PS(m), then for a given rule number r, pat(r)@p is
the (possibly "virtual") pattern element at path p for par(r).
Or more generally, pat@p is the pattern element at path p in the
pattern pat.

Alternatively, pat@p could be the (sub) _pattern_ at path p.

Top-match: v $ pat
For value v and pattern pat (of the same type), v$pat is true
if v matches pat at the top, i.e. the top "construction" of the
value agrees with the top construction of the pattern.

Defn: v $ x for any value v and variable x
      C v $ C pat  (for any v and any pat, which may both be null)
      (v1,..,vn) $ (pat1,...,patn)
      v $ (pat1 | pat2) if v $ pat1 orelse v $ pat2  ??!

Full-match: v $$ pat   (v and pat have same type)

Defn: $$ : value * pattern -> bool
      v $$ x =true  (x a variable, any v)
      C v $$ C pat = v $$ pat
      (v1,...,vn) $$ (pat1,...,patn) = v_i $$ pat_i (i = 1, ..., n)
      v $$ pat1 | pat2 <= v $$ pat1 orelse v $$ pat2

Note that $$ cannot be defined in terms of $, because of the OR pattern case.

Full-match can be extended to a partial matching function that returns
a variable instantiation environment:

Defn: env = finite map from variables to values, variable-value a-list

Defn %% : value * pattern -> env option
     v %% x = SOME {(x,v)}
     C v %% C pat = v %% pat
     vs %% pats = compose* (map2 %% (vs,pats))
     v %% pat1 | pat2 =
         case v %% pat1
	   NONE => v %% pat2
	   envOp => envOp

where compose* is the obvious composition of env options.


--------------------------------
37.6 Least matching rule/pattern
--------------------------------

Defn: LMR (v, pats) = the smallest index r (ruleno) such that v $ pats(r)

LMR(v,pats) is the ruleno of the first vlayer matching v.

Pattern at path: pat @ path : pattern
Defn: @: pattern * path -> pattern + {#}   (# is the "null pattern")
      pat @ <> = pat
      pat @ c = # if pat = c (a constant)
      pat @ n:path = pat_n @ path where pat = (p1, ..., pn)  (n a natural number)
      pat @ K:path = pat' @ path if pat = K pat' (K a non-constant constructor)

Defn: @@: value * path -> value
defined similarly to @

Paths do not explicitly represent alternatives in OR patterns, i.e. layers.
Layers are an independent "dimension".

Conjecture: Layers can be tracked by using layers instead of rule numbers in
the direct and default fields of AndOr nodes.

Given a path p in the pattern space and a rule r, what
are the possible relations between p and r.

(1) the pat(r) @ p is "generic" for the type of p
    For example, pat(r) @ p may be a tuple/record. This structure
    has to be matched by any v@p where v is of the pattern type.
    r cannot be contradicted by a value at p. Hence p is not "relevant"
    to r. But "relevance" is only important at choice (OR) nodes in the
    AndOr tree anyway.

(2) pat(r) @ p is an OR node constructor/constant (say for a
    non-singleton datatype). Given multiple choices for this node, r
    can be "invalidated" at p, meaning that there are values that have
    other constructors at p: v@p <> pat(r)@p), so p is "relevant" to r.
    [In this case, is the last element of p the constructor key?  Or could
    p be the path of the OR node itself?  If the later, then the "choice"
    at the OR node at p is not represented in p itself, but pat(r)@p would
    presumably include the constructor/constant key determined by pat(r).]
    [pat @ path] is a _pattern_, not an andor node, so it can only be
    a particular constructor subpattern, no the OR-node itself, which
    represents a choice of several constructors or keys.

(3) pat(r)@p is a variable, so pat(r)@p matches any v@p for v of domain type.
    pat(r) = x => v %% pat(r) for any v.

(4) pat(r)@p' is a variable for some p' < p (p' a prefix of p). In this case
    pat(r)@p' is dominated by a variable and so a value v@p will always be
    consistent with r, and pat(r) @ p is not "relevant".

Example(s):

    pat = (true, false)  (rule 0)
    pat @ 0 = true
    pat @ 1 = false
    pat @ 1:false = #0
    pat @ 0:true = #0  (the rule matched)
    pat @ 1:false = #0 (")
    pat @ 0:false = undefined

What about OR (layered) patterns?

    pat = (true | false, false)  (say rule 0)
    pat @ 0 = true | false
    pat @ 1 = false
    pat @ 1:false = #0
    pat @ 0:true = #0.0  (the layer matched)
    pat @ 0:false = #0.1 (")

A value matches a pattern sequence, pats, if there if it matches some pattern
in the sequence (i.e. LMR(v,pats) is defined).  But we need more information:
we need to know which _horizontal_ layer (if any) is matched, since this affects
the resulting variable bindings.


--------------
37.7 Relevance
--------------

For the simple (no OR-pattern) case, there is a relation

   relevant(path,ruleno)

between paths (in the overall pattern space, i.e. nodes in the AndOr tree)
and rules. 

In terms of the current AndOr tree structure,

   relevant(p,r) <=> r \in Node(p).direct & isOR$ (Node(p))

   (where isOR$ n if n is a non-singleon OR node).

and

   r \in Node(p).default => not relevant(p,r)

Question: Do we need direct and default fields for AND nodes?  direct for such
nodes is "automatic", to be assumed, because any value or rule pattern has to
match the tuple structure at that node.  But a default (variable) at such a node
is inherited by any descendent nodes.
   
Does this relevance relation extend naturally to layers instead of rules? Lets try.

relevant(p,r) iff the construct at pat(r)@p is refutable, i.e. it is a constant or
nonsingular constructor. This means that there exist values that do not match
pat(r) at p, i.e. values that "refute" pat(r) @ p.

When building the decision tree, we select OR nodes (choices) based on the fact that
they are relevant to the earliest (smallest) r that is still "viable".

Defn: r is _viable_ at a given AndOr node (or potential decisionTree node) if it
has not been "refuted" by a choice earlier on the path to the node.

Given a path p in the pattern space of a match, and a layer l what are the
possible relations between p and l

First problem is what is the meaning of pat(l) for a layer l?

(1) pat(l)

We need an simple/naive/abstract matching function that records layers, as
a specification of the matching problem.  First lets specify just rules,
or verticle layers, leaving aside OR patterns.

type env = (variable * value) list   (* association list representation of env *)

(* patmatch with no OR patterns *)
val patmatch : value * pattern -> env option
fun patmatch (v,x) = SOME [(x,v)]
  | patmatch (c, c) = SOME nil  (* c a constant: number, string, constant dataconstr *)
  | patmatch (C v, C pat) = patmatch (v, pat)   (* C a non-constant dataconstr *)
  | patmatch (Tuple vs, Tuple pats) = compose* (map2 patmatch (vs,pats))

val match : value * pattern list -> (env * ruleno) option
fun match0 (v, nil, r) = NONE   (* no patterns)
  | match0 (v, pat::pats, r) =
      case patmatch (v, pat)
        of SOME env => SOME (env, r)
	 | NONE => match0 (pats, v, r+1)
fun match (v, pats) = match0 (v, pats, 0)

The function match returns NONE if the value does not match any pattern,
or SOME (env, r) where r is the index of the first pattern that matches,
and env is the environment generated by the match.  The matching ruleno
could be used to dispatch the associated (r^th) rhs, instantited by the
environment.

Prop: If match (v, pats) ==> SOME (env, r) then domain env = vars(nth(pats, r))

Now let's extend this to handle OR patterns.

(* patmatch with OR patterns, sequential *)
val patmatchOr : value * patternOr -> env option
fun patmatchOr (v, x) = SOME [(x,v)]
  | patmatchOr (C v, C pat) = patmatchOr (v, pat)
  | patmatchOr (Tuple vals, Tuple pats) = compose* (map2 patmatchOr (vals,pats))
  | patmatchOr (v, pat1 | pat2) =
      case patmatchOr (v, pat1)
	of NONE =>  patmatchOr (v, pat2)
	 | SOME env => SOME env

val matchOr : value * patternOr list -> (env * ruleno) option
fun matchOr0 (v, nil, r) = NONE   (* no patterns)
  | matchOr0 (v, pat::pats, r) =
      case patmatchOr (v, pat)
        of SOME env => SOME (env, r)
	 | NONE => matchOr0 (pats, v, r+1)
fun matchOr (v, pats) = matchOr (v, pats, 0)

Note that the only change introduced by OR patterns is an additional clause in
matchOr0 to deal with OR patterns.  All the other cases remain the same as in
the simple pattern case.  In particular, there is no need for "layers", just
rule numbers.


---------------------------
37.8 Implementation Problem
---------------------------

How can pushDefaults push "layers" instead of "rules" down through the AndOr tree?


-----------------------------------------------------------
37.9 Note & Examples for top-level variables in OR patterns
-----------------------------------------------------------

Ex.1 (orpat/o5.sml)

  (1)  x | (_::x)
  (2)  (_::x) | x

These are two legal OR patterns of type t list (for some type t).
They both unambiguously bind the variable x : t list.
For (1), the left pattern x will match any value, so by left-to-right
priority, the right pattern (_::x) will never be matched. Thus the
relative path of the match for x will always be <>.  Thus

	 x | (_::x)  -->  x   ("simplifies or reduces to")

And in general we have the simplification pattern rewrite rule:

         x | p  --> x

for any legal pattern p. I.e. p will never match.

For (2), the only value that will match with the right pattern (x) is nil,
which does not match the left pattern (_::x).

  (0)    (_::x) | x => ...
  (1)    nil => ...

Here rule (1) will be redundant, since any value will match the pattern for
rule (0). So the right pattern x in (0) shadows, or defaults, rule (1).
How is this implemented when we do pushDefaults?  Do we use default rule numbers,
(0) in this case, or do we need default layers, (0.1) in this case?

Andor tree for (1): [assuming 1 rule (0)]

   OR (t list, <>, Vars [x @ 0.0])
     nil
       LEAF (direct = {}, default = {0.0 ?})
     ::
       AND
         0 WILD
	 1 VAR [x @ 0.1]

Here the "default" "ruleset" for nil is either {0} (a ruleset) or {0.0} (a layer set).
Is the extra information in the layer set relevant?

Decision tree for (1):

   DLEAF (0, ...)

Ex. 1a (orpat/o5a.sml)

fun f u =
    case u
     of ((_::x), (_::y)) => (x,y)
      | ((_::x), y) => (x,y)
      | (x, (_::y)) => (x,y)
      | (x, y) => (x,y);

Ex. 2

  (0)  (true, (_::x) | x) => rhs0(x)
  (1)  (false, nil) => rhs1

AndOr tree:

   <> AND (bool * t list)
      <0> OR (bool) (no Vars, AsVars)
          true
	    <0,true> LEAF
	  false
	    <0,false> LEAF
      <1> OR (t list) vars=[x @ 0.1]
          nil
	    <1,nil> LEAF(direct={}, default={0.1})
	  cons
	    <1,cons> AND(direct={0}, default={0.1})
	       <1,cons,0> WILD
	       <1,cons,1> VAR [x @ 0.0]

Decision tree:

   CHOICE <0>
      true =>
        CHOICE <1>
	  cons => DLEAF 0  (bind x at <1,cons,1>)
	  _ => SOME(DLEAF 0) (x default, bind x at <1>)
      false => 
        CHOICE <1>
	  nil  => DLEAF 1  (no x binding)
	  cons => DMATCH

Note: can't "unify" the two CHOICE <1> subtrees, both derived from the <1> OR node.

Ex. 3

  p = ((_::y) | y, (_::x) | x)

Matches any value of type (t1 list * t2 list).

  p # (nil, nil)  -->  {(y,nil), (x,nil)}
  p # ([a], [b])  -->  {(y,nil), (x,nil)}
  p # ([a1,a2], [b])  -->  {(y,[a2]), (x,nil)}
  p # ([a1,a2], [b1,b2,b3])  -->  {(y,[a2]), (x,[b2,b3])}
  
------------
orpat/o5.sml
----------------
fun f u =
    case u
     of (((_::x) | x), ((_::y) | y)) => length x + length y;
----------------

AndOr tree:

  AND <>;
    0 OR <0>; vars = [x @ 0.1]; dir = {0.0}; dft = {0.1}
      cons: {0.0}  (=> not defaulted by x @ 0.1)
	AND <0,cons>; dir = {0.0}; dft = {}
	  0 WILD <0,cons,0>; dft = {0.0}
	  1 VAR <0,cons,1>; vars = [x @ 0.0]; dft = {0.0}
    1 OR <1>; vars = [y @ 0.1]; dir = {0.0}; dft = {0.1} 
      cons: {0.0}  (=> not defaulted by y @ 0.1)
	AND <1,cons>; dir = {0.0}, dft = {}
	  0 WILD <1,cons,0>; dft = {0.0}
	  1 VAR <1,cons,1>; vars = [y @ 0.0]; dft = {0.0}

   Note: For x @ 0.0 and y @ 0.0, the orpaths are relative to two different
   	 OR patterns at paths <0> and <1> respectively.  Does this matter?
	 They can be differentiated by using both the path and orpath?

Decision tree:

  There are two "parallel" OR nodes at paths <0> and <1>.
  variants# <0> = 2 = variants# <1>
  defaults# <0> = 1 = defaults# <1>
  => <0> and <1> have the same priority.  Assume <0> chosen.

  CHOICE <0>
    cons: (relevant : <0>{0.0} -- but not to <0>{0.1})
      CHOICE <1>  (bind x @ <0.cons.1>)
      	cons: DLEAF 0.0 (bind y @ <1.cons.1>)
	      (relevant : <1>{0.0} -- but not to <1>{0.1}) 
	_ : SOME(DLEAF 0.1)  (bind y @ <1>)
    _ : SOME(DLEAF 0.1)  (bind x @ <0>)

   Layer uses (DLEAF):
     0.0 : 1
     0.1 : 2
   both layers belong to rule 0, so use the same rhs, which therefore
   has 3 DLEAF uses.

Absyn:

  let rhsf0 = fn (x,y) => length x + length y in
  fn v0 =>
   let (v1,v2) = rdest(2) v0
    in SWITCH v1
         of cons v3 =>
	      let (v4,v5) = rdest(2) v3
	       in SWITCH v2
	            of cons v6 =>
		         let (v7,v8) = rdest(2) v6
			  in rhsf0 (v5, v8)
		     | _ => rhsf0 (v5, v2)
          | _ =>
	     SWITCH v2
	       of cons v9 =>
	       	    let (v10,v11) = rdest(2) v9
		     in rhsf0 (v1, v11)
		| => rhsf0 (v1, v2)

where
   let (x,y) = rdest(2) z
    in ..

is short for

   let x = SELECT(0,z)
       y = SELECT(1,z)
    in ...

or

   let x = SELECT(0,z)
    in let y = SELECT(1,z)
        in ...

-------------------------------------------------------
Ex 3a

  fun f u =
      case u
        of (_::x, _::y) => x + y    (* rule (0) *)
	 | (x,y) => x * y           (* rule (1) *)

AndOr:

  AND <>; int list * int list, dir = {0,1}, dft = {}, vars = {}
    0: OR <0>, int list, dir = {0}, dft = {1}, vars = {x @ 1}
         cons:
	   AND <0,cons>; int * int list, dir = {0}, dft = {1}, vars = {}
	     0: WILD <0,cons,0>; int; dft = {1}, vars = {}
	     1: VAR(x) <0,cons,1>; int list; dft = {1}, vars = {x @ 0}
    1: OR <1>, int list, dir = {0}, dft = {1}, vars = {y @ 1}
         cons:
	   AND <1,cons>; int * int list, dir = {0}, dft = {1}, vars = {}
	     0: WILD <1,cons,0>
	     1: VAR(y) <1,cons,1>; int list; dft = {1}, vars = {y @ 0}

Decision Tree:
  The two OR nodes are symmetric, so have the same priority. So choose <0>
  first.

  CHOICE <0>
    cons: (relevant to rule 0, but not to rule 1)
      CHOICE <1>  (bind x @ <0.cons.1>)
      	cons: DLEAF 0 (bind y @ <1.cons.1>)
	      (relevant to rule 0, but not to rule 1)
	_ : SOME(DLEAF 1)  (bind y @ <1>)
    _ : SOME(DLEAF 1)  (bind x @ <0>)

    Rule uses:
      rule 0: 1
      rule 1: 2
    rule 0 has a single DLEAF use, and does not need to be abstracted
    rule 1 has 2 DLEAF uses, so have to define abstracted rhs rhs1

Absyn:

let rhs1 = fn (x,y) => x * y  -- rule 1 rhs; used twice)
 in  
  fn v0 =>   -- u
   let (v1,v2) = rdest.2 v0    -- v1 = #1 v0; v2 = #2 v0
    in SWITCH v1
         of cons v3 =>
	      let (v4,v5) = rdest.2 v3    -- v5 = tl v1
	       in SWITCH v2
	            of cons v6 =>    -- rule 0
		         let (v7,v8) = rdest.2 v6  -- v8 = tl v2
			  in v5 + v8     -- rule 0 rhs, used once, inlined
		     | _ rhs1 (v5, v2)   -- rule 1 rhs
          | _ => rhs1 (v1, v2)           -- rule 1 rhs

------------------------------
4a.  x | true  (not legal, no x in 2nd arg)
4b.  x | x as true => not x  [should reduce to x]
4c.  fn x => not x     (0)
      | true => false  (1)

     x in (0) pattern shadows true in (1) pattern
     => rule (1) is redundant

AndOr (4c):
  OR <>; bool; vars={x @ 0}; dft = {0}
    true:
      LEAF; dir={1}, dft={0}}

Decision Tree (4c): ?

  DLEAF 0 (bind x @ <>)

Absyn (4c): (?)  [Rule (1) should be flagged as redundant]

  fn v0 =>
     let v1 = v0
      in not v1

BUG[newmc]. No "match redundant" error message is produced.

BUG[newmc]. No "match nonexhaustive" warning message is produced by:

  - fn true => 3;


-------------------
37.10 More examples
-------------------

5. datatype d = A of int | B of int | C of int
5a.  A x | (B x | C x) => x + 1
5b.  A x | B x => x + 1  (0)
     C x => x + 2        (1)

AndOr (5a):

  OR d; <>; vars={}; dir={0}; dft={}
    A: VAR(x) int; <A>; vars={x @ 0.0}
    B: VAR(x) int; <B>; vars={x @ 0.1.0}
    C: VAR(x) int; <B>; vars={x @ 0.1.1}

Decision Tree (5a):

  CHOICE <>
    A: DLEAF 0  (bind x @ <A>)
    B: DLEAF 0  (bind x @ <B>)
    C: DLEAF 0  (bind x @ <C>)

  varenvMC = {(x, 0.0, <A>, v1), (x, 0.1.0, <B>, v2), (x, 0.1.1, <C>, v3)}
  (from bindSVars)

Absyn (5a):    

  let rhs0 = fn u => u+1
   in fn v0 =>
         SWITCH v0
	   of A v1 => rhs0 v1
	    | B v2 => rhs0 v2
	    | C v3 => rhs0 v3
  rhs0 used three times (on each leaf of the orpath tree)

-----------
AndOr (5b):

  OR d; <>; vars={}; dir={0}; dft={}
    A: VAR(x) int; <A>; vars={x @ 0.0}
    B: VAR(x) int; <B>; vars={x @ 0.1}
    C: VAR(x) int; <B>; vars={x @ 1}

Decision Tree (5b):

  CHOICE <>
    A: DLEAF 0  (bind x @ <A>)
    B: DLEAF 0  (bind x @ <B>)
    C: DLEAF 1  (bind x @ <C>)

  varenvMC = {(x, 0.0, <A>, v1), (x, 0.1, <B>, v2), (x, 1, <C>, v3)}
  (from bindSVars)

Absyn (5b):

  let rhs0 = fn u => u+1
   in fn v0 =>
         SWITCH v0
	   of A v1 => rhs0 v1
	    | B v2 => rhs0 v2
	    | C v3 => v3 + 2  -- rhs1 (x+2) used once

------------------------------
6. List OR patterns
6a.  x::y | x::_::y
6b.  [x,y] | [x,_,y]
6c.  [x,y] | [y,_,x]
6d.  [_,x] | [_,_,x]
6e.  ([_,y] | [_,_,y]) => y;  (orpat/o3.sml)

6a.
AndOr:

   OR (t list); <>; vars={}, dir = {0}, dft = {}
     cons:
       AND (t * t list); <cons>, vars={}
         0: x
	 1: y
------------------------------
7. redundant non-variable patterns
   (x,y) | (x,y)

Should this be pre-analyzed as a redundant OR pattern and reduced to (x,y)?
Or should the match be compiled in such a way that the redundancy is
detected during compilation and eliminated (possibly with warnings)?

Another variant:

7a. (x, y) | (y, x)

7b. (orpat/o2.sml)

   (y, _) | (_, y) => y;


8. (orpat/t1.sml)

  fun f x =
    case x
      of (([(v,_)], []) | ([], [v])) => 1
      
8a. (orpat/t3.sml)

  fun f x =
    case x
     of ([(v,_)], []) => v
      | ([], [v]) => v;

9. (orpat/t4.sml, fcontract.sml)

  fun f (([(_,le)],NONE) | ([],SOME le)) = 0;

10. (orpat/t5.sml)

  fun f x =
    case x
     of ([false, x, _] | [_, _, x]) => x;

  fun f x =
    case x
     of [false, x, _] => x
      | [_, _, x] => x;

11. (scc/t3.sml, from MLRISC/sparc/mltree/sparc.sml)

datatype t
 = LI of int
 | ADD of t * t
			   
fun addr (ADD ((ADD (e, LI n) | ADD (LI n, e)), LI n')) = n + n'


----------------------------------------------------
37.11 Pattern _overlays_ or _layers_ with priorities
----------------------------------------------------

Without OR patterns, each _rule_ overlays previous patterns with rule numbers
as priorities.

   p | q

analyze p at this point, then overlay q, at the _same_ point (path). For a
pattern like x as (p | q), secondary variable x is assigned to the common
path/node occupied by p and q (and any other previous or subsequent layers).

With OR patterns, the priorities are expressed as (ruleno * branch list), where

     datatype branch = Left | Right
     (or type branch = int (* using only 0 or 1, representing bits *))

branches are left or right branches at "Split" nodes (i.e. "OR patterns"). The
ordering of priorities is

     (rule1, branches1) < (rule2, branches2)
     iff rule1 < rule2 or rule1 = rule2 and brances1 < branches 2

where the ordering on branches (= branch list) is 

     Left < Right
     branches ordered lexiocographically


------------------------------------------------------------------
37.12 Preliminary Implementation Ideas for OR ("layered") patterns
------------------------------------------------------------------

1. View a pattern containing OR patterns as a tree where the nodes are the OR (or ;)
patterns.  Create "addresses" for the elements of OR patterns in the form of a list
of numbers, with the first element being the rule no, and the successive numbers being
the number within each nesting of ORs.

Extend the priority scheme for OR nodes to use these lists (i.e. OR tree addresses).

2. pattern continuations (at decision-tree and code level) for reuse instead of
duplication of decision tree subtrees.


------------------------------------------
37.13 Note on AndOr nodes and rules/layers
------------------------------------------

[** Simple Case: no OR patterns **]

At each node of the AndOr tree of a pattern list, there is a set of rules
(represented by rule numbers, ruleno) whose patterns are "viable" or "live"
at that node.  A rule/pattern is in this set (say liverules, or live for short)
if it is not "refuted", "contradicted", "knocked out" by one of the choid keys
in the path of/to the node.  Or in other words, the pattern is "consistent" with
the path.

A pattern is consistent with a path if the path is a member of the
path set (see "pattern space" above) that is the "semantics" of the
pattern. This will be the case if

(1) at each choice key along the path the key constructor/constant agrees with
the corresponding constructor in the pattern, or

(2) some prefix of the path agrees with the pattern, an at the corresponding
point in the pattern there is a variable (/wildcard).

Example:

  (1) cons(x,     nil)
  (2) cons(true,  cons(y,    nil))
  (3) cons(false, cons(true, z))
  (4) u

The AndOr tree is:

  <> OR(list) cons {1,2,3; 4}, vars = (u,4) (partial)
     <cons> AND {1,2,3}
                <cons:0> OR(bool) {1,2,3; 1,4},  vars=(x,1)
		      <cons:0:true> LEAF {2; 1,4}
		      <cons:0:false> LEAF {3; 1,4}
		<cons:1> OR(list) {1,2,3; 4}
                      <cons:1:nil> LEAF {1; 4}
		      <cons:1:cons> AND {2,3; 4}
 		         <cons:1:cons:0> OR(bool) {2,3; 2,4} (vars = (y,2)) (partial)
			    <cons:1:cons:0:true> # {3; 2,4}
		         <cons:1:cons:1> OR(list) {2,3; 4} (vars = (z,3) (partial)
			    <cons:1:cons:1:nil> {2; 34}

Now consider the pattern

  (2) cons(true,  cons(y,    nil))

It is live at path <cons:0:true> because the cons and true keys agree between
the pattern and the path.  It is dead at <cons:0:false> because the key false
in the path does not agree with the corresponding constructor true in the pattern.

Pattern (2) is also live at path <cons:1:cons:0:true> because the two keys cons, cons
agree between the path and the pattern, and the pattern has a variable (y) at the
position of the true constructor in the path. I.e. (2) @ <cons:1:cons:0> = y, so
a prefix of the path <cons:1:cons:0:true> designates a variable 

So liveness could be calculated directly from a pattern and path instead of being
recorded in the AndOr nodes's direct and default fields.  But in any case, it appears
that we do not need to maintain the separate direct and default rule sets, since
what we actually need to know is the live set of rules at a path (node).

Currently we need a post-processing pass, pushDefaults, to manage the rules that are
live because of variable defaulting.  This is necessary because the variable that
causes defaulting may be added by a late rule after AndOr subtrees have been
constructed.

While constructing a decision tree (makeDecisionTree), at a leaf node (when there
are no relevant OR nodes), we assign the least live (survivors) ruleno.  Then in
MatchComp..genRHS we use that rule number and a varenvMC environment to produce
a pvar -> svar environment/substitution to be applied to the original rhs expression
in MatchComp..transExp.

There is some redundancy in the representation of rule defaulting, since a defaulted
rule (for an AndOr node and its subtree) is represented in the defaults fields (for
all nodes in the subtree), and also in the vars field of the node where the variable
occurs.

What we need when we reach a decision tree DLEAF node is the minimal live ruleno
(R.minItem survivors). Note that survivors is passed as an argument to makeDecisionTree,
but it is computed for one recursive call based on _getLive_ applied to variant andors,
and for the other recursive call based of defaults of the current OR node.

Probably we just need to record a single "live" ruleset at AndOr nodes.  This would
be, for each node, the union of the current direct and default sets.

[** Case with OR patterns and layers **]

With OR patterns, we need to have layers to calculate the pattern variable
translations (which depend on layers, not just rules). This means that DLEAF
must provide the minimum layer rather than rule, since the DLEAF ruleno
was what was passed to genRHS and then to bindSVars to be used together with
the varenvMC to choose the right svar for each pvar.

Thus we need to compute and/or record the set of "live" layers rather than
live rulenos for AndOr nodes.

There are two issues relating to comparing/ordering layers.

(1) hlpath prefix relation -- is it relevant, or can we regard lvpath prefixes
as "equal" for the purpose of layer order?

(2) layers can be "redundant" in a given (rule) pattern. Two "independent" top-level
OR patterns give rise to parallel and independent sets of layers with the same
representation. Will these ever get confused?

* Problem 1: details of how the "live" layer set is propagated down through the
AndOr tree, or even whether it should be included in the AndOr tree or calculated
on the fly.  We could add either layer(s) or perhaps just hlpaths to AndOr tree
nodes if that would help.  Note that the notion of "a layer being live at a node"
has to be defined carefully so that we get the right layers at DLEAF nodes in the
decision trees. Then we need to insure that this layer will produce the right
pvar -> svar substitution in MatchComp..bindSVars.

* Problem 2: when adding members to layer sets or testing membership in layer sets,
need to verify that the "fuzziness" of layer equality (associated with equality
of hlpaths of different lengths) does not cause problems.  E.g. a layer set contains
(r, 0:0:1) and we insert a layer (r, 0:0:1:0), which is "equal" to (r, 0:0:1),
after which the layer set still only contains the (r, 0:0:1) layer.
Can this matter?  If so, need an example to show what can go wrong, and a fix.

For choosing svars for pvars, we need as long an hlpath as necessary to cover
the pvar. For example:

      (_::_::x | _::x )  |  x

we have

      x @ (0,1)   (rhs)
      x @ (0,0.0)
      x @ (0,0.1)

If the varenvMC just maps x @ (0,1) to s, and (0,0.0) and (0,0.1) are both
considered "equal" to (0,1), then ... this counterexample doesn't work! The
"earlier" or "higher" binding for x would have to be at a prefix of both
0.0 and 0.1, i.e. 0. But it is not possible(?) for a variable to bound at
multiple points along a single given hlpath!  Two occurences of pvar x _must_
be associated with "unequal" (diverging) hlpaths. Otherwise the following
rule would be violated:

RULE: Layer-Binding
-------------------
A pattern variable can be bound at most once in an hlayer (but it may be
bound multiple times in different, _related_, hlayers). [An hlayer is
determined by a maximal hlpath.]

It is also the case that if x is bound multiple times, the multiple times
must be in "related" layers.

Defn: The _root_ of a horizontal layer is the path to the OR-pattern in
the context of the rule pattern.

Defn: Two horizontal layers are _related_ if they have a common root.

Defn: A layer (r,p) is "maximal" if there does not exist a layer (r, p') where
p < p' (p is a proper prefix of p') [in the context of all layers occurring
in an AndOr tree of a pattern list].

Note: The "best" hlpath is the longest hlpath, since it contains as prefixes
all the "compatible" hlpaths that are considered EQUAL when comparing hlpaths.

Example: 

       (_::x) | x

The left occurrence of x (@ path cons:1) is associated with layer (0, 0)
and the right occurrence of x (@ path at path <>) is associated with layer (0,1).

Example:

(1)    ((_::x | x) , (_::x | x))

Here the root path of the left OR-pattern is <0> and the root of the
right OR-pattern is <1>. They have different roots so they are not
"related".  Thus the pattern is illegal because it has multiple
occurrences of x in unrelated layers. The following pattern would be
legal.

   ((_::x | x) , (_::y | y))

Note: If the Layer-Binding rule is enforced (e.g. by prior checking of the
patterns), then it will never be the case that a pvar is "bound" in a varenvMC
to layers with different roots.

Note: This notion of related layers needs to be generalized (recursively),
since within an OR-pattern, there may be _unrelated_ subsidiary OR-patterns.
(need "relative roots" of families of layers?)  For instance, pattern (1)
above may occure nested within a branch of an OR pattern (i.e. within a
single horizontal layer). But it will still be the case that the root paths
of the two problematic OR patterns are different.  

Prop: Two related layers will have the same set of pvars.
[Assumine the layers are "maximal"?]

Prop: A variable x will occur at most once in a given layer.  If x
occurs in layer (r, p), then x will not occur at (r, p') for any p' >
p (p' extending lhpath p).  Any other occurrence of x (in pat(r)) must
be associated with layer (r, q) where p and q diverge (are not EQUAL
in the hlpath ordering hlpathCompare, i.e. neither is a prefix of the other).

Conjecture: if a varenvMC maps a pvar x to a given (layer, svar), then that layer
will be "unequal" to the layer of any other binding of that pvar.  Otherwise
there would have been two bindings of x in the same layer. Another way of putting
this is that a pvar x maps to a list of possible (layer,svar) bindings where all
the layers are unequal (different, divergent).

* Problem 3: need to generate a few carefully designed test cases to illuminate
these two issues with layer and layer set "functionality".

* Problem 4: need to rethink downward propagation of live layer sets (compared
with the current downward propagation of default rule sets.  Interaction of
matching keys (direct) and variable-caused defaulting with layers.

Assert: A layer can be "live" at an AndOr node but not live at a descendent of
that node. (Example?).

Question: What role do "traces" (path lists) play in this, if any?  Main motivation
for adding traces was to use them to generate counterexamples at DMATCH nodes in
decision trees.


----------------------------------
37.14 "Intersection" of layer sets
----------------------------------

For the purpose of calculating "intersections" of layer sets, we can regard
a rule (layer (r,nil)) and matching any layer of the form (r,p).  Similarly,
(r, p) will match any layer of the form (r, p') where p < p' (and (r,p') can
replace (r,p) in the result of intersection.  So layer sets can be effectively
seen as "upward" closed with respect to the prefix ordering of lhpath components.
Thus LS.intersect may result in a larger set than either of its arguments.

A minimal representation of an LS.set may then consist of layers with maximal
lhpath components, since these will cover the corresponding prefixes.  Should
LS.intersect maintain this representation?  Or can it produce representations
with "redundant" layers?  Does it matter?

Thus repeated intersections with "live" layer sets of nodes will develop a set
of (eventually) maximap layers, possibly with additional "redundant" members.

The maximal hlpath should also be used when calculating the "least" element
for a layer set, i.e. the least element should have the least rule and for that
rule the least, maximal hlpath.

--------------------------------------------------
37.15 Layers vs or-expansion ramification of rules 
--------------------------------------------------
[April 25/2021]
Terminology: revmc refers to the revised version of the "old" (Aiken) match compiler
where a preprocessing phase "or-expands" or "ramifies" the patterns (and consequently
the rules of the match).

After much effort at "simplifying" the management of layers to achieve something
like the ramified rules produced in the revmc verion, it looks like the ramified
patterns/rules approach of revmc already achieves the desired simplification of
the layer encoding, i.e. the interpolation of horizontal (or=pattern) layers between
the vertical rule layers determined by the sequencing of rules in the match.

Therefore it seems that the correct coarse is to abandon the layer idea and its
complications and stick with the revmc approach, but adapting it to the newmc
framework of translating matches at the absyn level, pre-translation phase.

The andor and decisiontree phases are based solely on the (ramified) patterns,
so there is no necessary difference in those phases between translating to
absyn or translating to PLambda.lexp.  But note that revmc can do without tracking
types in the andor and decisiontree data structures.  The necessary types in the
lexp translation amount to (a) the pattern, i.e. lhs type, (b) the result, i.e. rhs type,
and vector element types for vector patterns (why?, how are these used?).  The lhs
and rhs types can be treated as "global" type parameters for a given match and passed
as parameters to the top-level match compile function (matchComp).  These types are
provided in the abstract syntax via the revised "fnrules" type, which supplies lhs
and rhs types for the rules.

-------------------
Note: Actually, the revmc version only depends on the rhs (result or return) type,
which is only needed when translating to (or introducing) RAISE lexps that indicate
match mailure (raising a Match or Bind exn, or reraising a non-matched exception in
an exception handler match.

If we do andor and decision tree construction without detailed types (i.e. types at
each andor node, then when generating absyn code from the decision tree we will need
to reconstruct the detailed types (i.e. the types for the administrative variables
bound to intermediate values during matching).


================================================================================
[38] polymorphism, bound type variables
================================================================================

The problems:
  (1) how to determine and track type variable bindings and their associated
      occurences within types embedded in expressions.
  (2) How to determine the type arguments used to "instantiate" polymorphic
      variables (TCAPP).

These issues have to be treated consistently in the front-end absyn and FLINT
lexp (plambda/flint) representations.  What can be done at the absyn - typecheck
level and what is handled in translation to plambda.

Another issue is that in plambda/flint, type variables and their bindings are
represented in two different ways.

  (1) by deBruijn indices (depth and index in a tuple of type variable bindings),
  (2) by type-levelo lvars (after deb2names transformation)

These to forms of type variable representation coexist to some extent.

The elaboration and type checking phases need to be revised to treat type
variable abstraction and type variable references (in types and expressions)
is a consistent, "first-class" way.  This will involve reconsideration of the
machinery used to determine binding points of implicitly bound, explicitely
occurring type variables in expressions.`

Questions:

1. What is the relation between the boundtvs field of VB and the btvs field of
individual bound variables (i.e. pattern variables). In the case of simple
VB bindings with a variable pattern, are they the same?

2. In boundtvs and btvs, what is the significance of the order of variables?  And
can they differ between "equivalent" boundtvs and btvs.

Examples:

1. let val (x::y, u::v) = e
    in ((x,u), (y,v))
   end	       

Two type variables, four value variables.
Matchcomp translation: (Wrong -- not using ptuplevar)
  
   let a = e         -- what's the type of e?  Let's assume e : 'a list * 'b list
     let b = a.1
       let c = a.2
         switch b   -- based on OR priority
	   nil => FAIL (BIND)
	   :: d => let e = d.1    -- x
	             let f = d.2  -- y
		       switch c
		         nil => FAIL (BIND)
			 :: g => let h = g.1    -- u
			           let i = g.2  -- v
				     ((e,h), (f,i))

   a,b,c,...,i are administrative variables, assumed not to be polymorphic
   what are their types?  [Assume 'a and 'b bound in an outer scope, determined
   by a depth parameter d?.]

   a : 'a list * 'b list  ('a and 'b bound outside the outermost let)
   b : 'a list
   c : 'b list
   d : 'a * 'a list  -- decon of b (:: case)
   e : 'a
   f : 'a list
   g : 'b * 'b list  -- decon of c (:: case)
   h : 'b
   i : 'b list
   ((x,u), (y,v)) = ((e,h), (f,i)) : ('a * 'b) * ('a list * 'b list)

Lexp translation (?)

-------------------------------------------------------------------------------

Removal of wrapRECdec function.

The wrapRECdec function in ElabUtil (and associated functions) has been removed.
Connected with this was the fact that the RVB case of Typecheck.decType0 did not
generalize the types of recursively defined variables, so this case was modified
to restore the generalization of _val rec_ declarations, making them (potentially)
polymorphic.

A side effect of this (going back to directly and fully type checking val rec
declarations) is that Translate produces explicitly polymorphic declarations
for the declarations within a FIX lexp. Unfortunately, FlintNM.norm (FLINT/plambda/flintnm.sml),
which translates plambda lexps into the Flint IL, assumes that the declarations in
a plambda FIX are of simple, non-polymorphic functions, and it fails on a simple
example like

  fun f x = x

where the resulting val rec typing of f is polymorphic ('a -> 'a).

Since it is likely that this assumption of nonpolymorphic FIX bindings hold
in many places throughout the FLINT optimization phases, rather than "fixing"
FlintNM and multiple other similar places in the FLINT code that may be relying
on this assumption, the proposed fix is to "restore" the effect of wrapRECdec
during the translate phase, effectively moving the polymorphism to wrapped
VALdec redeclarations of the recursive function names.

The main issue in doing a Translate version of wrapRECdec is to manage the
polymorphic bindings of type variables (TFN) and related instantiations
(TAPP) of those type variables.

Algorithm Sketch
----------------

1. typecheck RVB bindings as usual, producing (potentially) polymorphic function
bindings (with the function absyn expressions containing "bound" metatyvars).

2. determine the polymorphic signature of the function tuple (how many bound
metatyvars, list of bound metatyvars).  Let ft be the variable bound to the
function tuple. Determine its polymorphic type.

3. determine the mapping between the bound metatyvars (or IBOUNDs) of ft and its
type and the metatyvars of the individual functions of the RVB.

4. Updata the DB indexes of the tyvars contained in the function expressions to
be those of the ft bound type variables. This will give the boundtvs to be used
when translating the RVB.

5. Build the new (lexp) declaration structure around the RVB structure (declaring ft
and the function rebindings to selections from ft). Translate these with the
appropriate type abstractions and type arguments (TAPP args) for ft, as in the
old mkVB. The type arguments will be based on a mapping from btvs tyvars for each
function to ft type argument indices.

See function transRVBs in FLINT/trans/translate-new.sml for the new implementation that
wraps VALRECdec rvbs to avoid explicit polymorphism in the rvb bindings after
translation.


================================================================================
[39] Different approaches to calculating "defaults" rule sets
================================================================================
Terminology: _revmc_ refers to the revised "old" (Aiken) match compiler.

The revmc approach to calculating defaults rule sets, which are associated only with
OR nodes, is "subtractive".  The set of "live" rules is maintained while building
the andor tree and at each OR node, all of the "direct" rules associated with
variants (the rules component in the (con, rules, subcase) structure of a variant)
are subtracted from the inherited live set to produce the defaults set.

The newmc approach is variable binding based and is "additive".  It requires distinguishing
"primary" variable bindings at and andor node from "scondary" (or "as-bound") variables,
and it uses only the primary variable bindings.  In a second phase of andor tree construction
(the pushDefaults function), it propagates a "defaults" rule set down
through the andor construction, adding the rules for primary variable bindings at each node.

CLAIM: These two approaches produce the same set of default rules at each OR node.

This has been verified by hand construction of andor trees for a couple of non-trivial
examples, but it would be good to produce at least an intuitive argument or proof sketch
for the claim.  A stronger empirical support would be to implement a combined andor
phase that used both approaches and compared the results. This could be used to compile
the compiler to a fixed point, which would thoroughly test the claim.

To get such an implementation starting with the revmc andor phase, one would need to
separate the varbindings in the simpleAndor representation into primary and secondary
(AS-bound) lists of varbindings, or add constructors to distinguish the type of the
individual (var, rule) bindings, so that the "additive" approach could distinguish
the primary bindings. For example,

    datatype varbinding = PRIMARY of (var * ruleno) | SECONDARY of (var * ruleno)
    type varbindings = varbinding list


================================================================================
[40] Path relevance, admin/destruct lvar bindings (theory)
================================================================================

40.1 Basic definitions.

DEFN: A pattern element is _relevant_ if it either _discriminates_
(can't be matched by all values of its type) or _names_ (i.e. binds
variables to destructs of a matching value). Matching an irrelevant
pattern has no effect, i.e. no case discrimination is made, and no
variable is bound.

Examples:

  x is relevant (variable binding, even though no destruction)

  _ is irrelevant

  () is irrelevant  (irrefutable, no variables)

  C is irrelevant (if C is the sole constructor of, e.g., datatype t = C)

  C _ is irrelevant (if datatype t = C of ty)

  (_, _) is irrelevant

  cons (_, _) is relevant (nil refutes it, so it discriminates).

  cons (x, xs) is relevant (it both discriminates, and binds variables x and xs)

DEFN: A path (in the pattern space, or, equivalently, the andor tree) is _directly relevant_ if

  (a) A (source) pattern variable is bound at that point, or

  (b) A refutable _case_ must be tested/discriminated at that point.

Conversely, a path is irrelevant if the pattern at that point is irrefutable, meaning
that it is either

  (a) a wildcard pattern "_", or

  (b) a singleton constant datatype constructor, or

  (c) a singleton dataconstructor applied to an irrelevant argument pattern, or

  (d) a conjunction (AND) of irrelevant patterns.

In other words, a path is relevant if the subpattern that it designates is either
refutable, or contains pattern variables.  The value matching at an irrevant path
is neither tested (examined) nor "named" (bound to an lvar).

PROP: A prefix of a relevant path is also relevant.

PROP: A prefix of an irrelevant path may be relevant, if it is also a prefix of
a relevant path.

All relevant points/paths need to have an administrative (or destruct) lvar bound
to the matching value (component).  This lvar will either

(a) "represent" a corresponding pattern variable in the match result record, getting
    bound to the matching value, or
(b) be tested to discriminate a branch of a case/switch/OR node, or
(c) be needed as a "stepping stone" to compute the value needed for (a) or (b).

[We could call lvars for the (a) and (b) cases "primary", and lvars needed for (c) "secondary".]

Given the full andor tree, there is a "relevant" subtree (subset of paths) containing
only the relevant branches (including their prefixes).

PROP: The number of destruct lvars needed is equal to the number of nodes in the
relevant subtree.

WRONG? There are also the destruct lvars associated with a case descrimination for constructors
with arguments (unless the argument pattern of the constructor is irrelevant, as in

     cons (_, _)

where the argument pattern "(_,_)" is irrefutable and contains no variables. Thus for
the (sub)pattern

     cons (x, y)

the destruct lvar will be bound to the pair (x, y) and will be assiciated with the
"cons" link in the relevant paths to variables x (p.cons.0) and y (p.cons.1).

The relevant paths can be stratified by their lengths.

At any given relevant path node,

(1) we can assume that there are lvars bound for each of the prefixes of the path back
to the root (with a nested sequence of LET (lvar ...) or SWITCH case bindings.


--------------------------------------------------------------------------------
40.2 Implementation Note:

  * Representing paths in reverse order (root last) makes it easier to derive prefixes
  (just repeated tl).

  * path as list of links/keys vs path datatype with path links?

  * may be useful to represent path as pair of length and link list?
    If, for instance, we need to derive a prefix of a specified length (how do we
    calculate how many "tl"s to take?

In the generate phase, when deciding when to introduce destruct mvar(s), for a given
path p, the needed information includes:

   Is p relevant?
   If so, what is the lvar associated with tl p (p's immediate prefix).
   What is the last path link to p.
   Is p terminal (in dectree?).

If p is _not_ relevant, we should generate no matching code at p, and there should be no mvar(p).

If p is relevant[pvar binding], then we need to generate an lvar and associate it
with pvars bound at p (?).

If p is relevant[case], where case is refutable (multiple con values), there needs
to be a mvar(p) to be the subject of the SWITCH, and for each relevant(?) branch there
needs to be a con destruct lvar for that branch. E.g. for p a list case, there
needs to be an mvar(p.cons), but not an mvar(p.nil), since the "argument" of nil is
"notionally" unit, which is irrefutable and pvar-free.  So a new mvar needs to be
generated to represent the "payload" of the cons constructor, i.e. the (head,tail)
pair -- but only if the (head,tail) pattern is relevant.

--------------------------------------------------------------------------------
40.3 Destructing ANDs

If p is an AND node, there will need to be destruct mvars for the successor paths
p.0, p.1, ..., p.n (where n is the number of children), but _only_ for those subpatterns
that are relevant (var-containing or refutable).

An AND path is relevant if _any_ of its children are relevant (because p is a prefix
of the path of each child).

--------------------------------------------------------------------------------
40.4 Per-rule relevant nodes/paths

The set of relevant nodes may change depending on what rule "fires" in a match.  For
instance,

   case e
     of (_, nil) => 0
      | (x, _) => x + 1

Here if rule 0 fires, then path <0> is _not_ relevant. But if the second component is
non-null and rule 1 fires, path <0> is relevant because of the binding of x at that path,
in that rule.

So the notion of relevance should be per rule, i.e. it is a relation on rules * paths.

   relevant (r, p)

When deciding on whether to bind an mvar (destruct variable) at a given point, it
should be deciced based on what rules are still "live" at that point.  Thus if the
decision tree looks like:

  OR <1> {0,1}
    nil {0}  <-- here binding x is not relevant
    ::  {1}  <-- here binding x _is_ relevant

We could pre-compute the relevance relation and use it when deciding where to
introduce mvar bindings.

--------------------------------------------------------------------------------
40.5 Two kinds of mvars

(1) mvars that will correspond to, and represent, pvars occurring in the lhs pattern
(only for such pvars that are "still relevant", i.e. whose rules are still live.
Call these "binding" mvars.

(2) mvars that bind to the subject of a non-trivial (multiple branch) choice.
These are relevant to rules that are live at that choice node.  Call these "choice"
mvars.

PROP: if relevant(r, p) and prefix (p', p) then relevant(r, p').  I.e. a prefix of
a path relevant for r is also relevant for r.

Defn: varpaths (r) = {p: path | pat(r)@p = VAR (*) }

The binding mvars are dependent on the patterns of rules.

The choice mvars are dependent on the decision tree structure.

Example

    (_,    nil) =>  0
    (u::v, _  ) =>  1

The choice point <0> is not relevant if we are under the nil branch for <1> (i.e. rule 0
is not live). Or in other words, not(relevant(0, <0>)).

The relevant relation naturally extends to rulesets:

    relevant(R, p) <=> (E r <- R) relevant(r, p)

Defn: relevant (r, p) if either
      (1) isVar(pat(r)@p)
      (2) @p is a choice node and r in live(@p)

Notation: If p is a path in an andor tree A, then A@p is the node or subtee of A
designated by the path.  If A is understood, the we can say @p, designating the
node of A at p.  (p in paths(A)).  A@p is undefined if p is not a path of A.

--------------------------------------------------------------------------------
40.6 Some Basic Functions

prel0 : rule * path -> bool  (path is var relevant _if_ rule is "live"; r in LiveRules)
crel0 : rule * path -> bool  (path is choice relevant _if_ rule is "live")

prel : ruleset * path/nodeId -> bool   -- pvar binding relevance
crel : ruleset * path/nodeId -> bool   -- choice node relevance

prel(R, p) = (E r <- R). prel0(r,p)     [ "<-" == "in" ]

rel : ruleset * path/nodeId -> bool
      rel (R, p) = prel (R, p) orelse crel (R, p)

So relevance in not simply an attribute of a path or nodeId (or node in the andor tree).

"Generators" for prel and crel relations could be represented as mappings:

   prelRules: path/nodeId -> Ruleset

s.t. prel(R, p) <=> R subset relRules(p)
                    or p < p' s.t. prel (R, p)

Q: How can we precompute the mappings prelRules and crelRules?

Q: How to propagate relation upward to path prefixes?

               --- p1 --- p2
                     \
		      \
		       p3

Suppose p1 is a prefix of p2 and p3.  Then 

       prelRules(p1) superset union(prelRules(p2), prelRules(p3))

in general

       p1 < p2  ==> prelRules(p1) superset prelRules(p2)

and

       prelRules(p) = Union {prelRules (p') | p < p1'}


--------------------------------------------------------------------------------
40.7 Possible Useful Auxiliary Functions

pathToId : (A: andor, p: path) : nodeId   = nodeId(A@p)

pathToPrefixIds : (A: andor, p: path) : nodeId list  == nodeIds of prefixes of p (in A)

prelMap : nodeId -> ruleset  (finite map from nodeId to set of rules for which it is prelevant)

crelMap : nodeId -> ruleset  (ditto for crelevant)

prefixMap : nodeId -> nodeId list  (nodeIds of nodes on prefexes of the nodeId path)
(the predecessor nodeId list could be built into the nodes during the construction of
the andor tree, or they could be used to populate a separate finite map).

Both prel and crel relations depend on dectree as well as andor tree. The pvar nodes
and their paths and prefixes can be determined from the andor tree, but we need knowledge
of "live" sets of rules as well, and these come from the dectree.

-----------------------------------------------------------------------------------------
40.8 Implementation sketch

1. For each andor node, we need to be able to access its predecessors/ancestors. 
This can be done by retaining the path as part of the node, and retracing the
(prefixes of the) path to access its ancestors.

ASSUMPTION: if a variable appears in the pattern of a rule, that rule is considered
relevant to the position of the variable (no "hidden" variables in a pattern). This
seems obvious.

2. We maintain a node environment that maps the id of a node to a ruleset R containing
the rules such that (A r <- R) relevant(r, id).  This is the "relevance set" of the node
(in this particular environment). These environments are not static, but are constructed
incrementally during the code generation phase.

3. There is a "static" part of the environment, dependent only on the andor tree, that
tracks the variable binding relevance property.

For each rule r, and node n @ p at which a variable is bound in pat(r), r is in the
relevant ruleset for n/p and is also added to the relevant ruleset for each ancestor node
of n (or each prefix path of p).

4. The remainder of the environment depends on the decision tree.  If rule r is "live" at
a choice node n @ p, then r should be added to the relevant ruleset for n/p and all its
ancestors.  But what doesn "live" mean in this case?  Does it only count if r @ p is a
constructor (or choice discrimant)?  What if r is "live" by default, because r @ p is
a variable (or WILDCARD)?  If it is live because r @ p is a variable, then r will be in
the relevant ruleset because of the variable binding anyway.  If r @ p is a WILDCARD,
then (I assume) r should not be considered relevant for p (~ relevant(r,p)).

Consider the example:

(0)    (true, 0)
(1)    (x, 1)
(2)    (_, 2)

Then we have

     relevant (0, <0>)    -- choice
     relevant (1, <0>)    -- pvar
   ~ relevant (2, <0>)    -- WILDCARD

so

     relevant (<0>) = {0,1}

If the "current live rules" has a non-empty intersection with {0,1}, then we should
generate an mvar and let-bind it appropriately.

Q: how do we determine what a new mvar binding should be?

     LET (newmvar, ???, ...)

This depends on what is being destructed, e.g. a record (AND node).

--------------------------------------------------------------------------------
40.9 NOTE: Saturated CHOICE/SWITCH

A CHOICE node may be saturated, meaning that all the datatype constructors occur
as discriminators in its cases component, but there may still be a default, e.g.

(0)   (true, false)  => rhs_0
(1)   (false, false) => rhs_1
(2)   (x, true)      => rhs_2(x)

Here <0> is an OR/CHOICE node, and both bool constructors are present, but if
<1> is true, rule (2) will be selected and will bind x.

--------------------------------------------------------------------------------
40.10 NOTE: variable relevance

There may be matches where a variable exists but is "irrelevant" or "unreachable".
But this seems to be possible only when the variable occurs in a redundant rule,
as in the following example:

(0)   true  => rhs_0
(1)   false => rhs_1
(2)   x     => rhs_2(x)

Here matching will never need to bind x, because rule (2) is redundant and will
never be reached during a match (i.e. no RHS 2 node in the decision tree).

CONJECTURE: If there are no redundant rules, then every variable is "relevant"
to some rule.

If this is the case, we may as well assume that for every pattern variable
v occurring at path p, there is a rule r s. t. relevant(r,p).

    (A v <- varpaths) (E r) relevant(r, p)

where

    varpaths = {path <- paths(andor) | is_AND (node(andor,path))

QUESTION: relation between cases and default in SWITCH (dectree.SWITCH)?

If the cases are saturated, under what circumstance would the default be used?  For instance,
if the subject type is bool and there are true and false cases, could the default ever 
be chosen?  [See 40.9 above]

CONJECTURE: if a SWITCH is sturated, default should/will be NONE.

--------------------------------------------------------------------------------
40.11 NOTE: OR/CHOICE rules

The rules for which an OR/CHOICE node (@ path p) is relevant fall into 3 categories:

(1) conRules: rules that have a con at p (isCon(pat(r)@p).  ("con-relevant" rules)
(2) varRules: rules that have a variable at p (isVar(pat(r)@p))  ("var-relevant rules)
(3) wildRules: rules that have a WC at p (isWild(pat(r)@p)); these are _not_ relevant to p. 

relevant(r,p) holds for (1) and (2), but not for (3).

   relevant (p) = conRules ++ varRules

where relevant (p) = {r | relevant(r,p)}


================================================================================
41. Wildcard pats and live rules
================================================================================

Notation: given a "match" M (i.e. a list of (pat, exp) pairs), if r < |M|,
then

    pat(r) = #1 (nth (M, r)).
    pats (M) = map #1 M
    andor (M) = andor (pats M)
    at (pat, path) = pattern at position p in pat, or undefined (also denoted pat@path)
    paths (pat) = {p : path | pat@p is defined}

In addition to the relevance relation (for a given andor tree):

    relevant (r: ruleno, p: path)

we also have a compatibility relation, stating that rule r is "compatible" with the
pattern node at path p:

    compat (r: ruleno, p: path)   <==>   pat(r) @ p is defined

and a corresponding ruleset mapping

    compat (p: pat) : ruleset = {r : ruleno | compat (r, p)}

And, conversely,

    compat (r: ruleno) : path set = {p : path | compat (r, p)}

PROP:  compat (r) = the set of all paths in pat(r)

(Another plausible name for this relation would be "compat".)

Notation: for a match/andor, pat(r,p) is the pattern at path p in rule r.

PROP:
(0) relevant (r, p) ==> compat (r, p)
(1) pat(r, p) is VAR ==> relevant (r, p)
(2) pat(r, p) is WC ==> compat (r, p)
(3) pat(r, p) is con ==> compat (r, p)   [ con = const | con pat ]

NOTE: "live" rules can be _static_, as tracked during construction of the andor tree
(andor.sml), where the notion is dependent only on the structure of the patterns or
andor tree, or _dynamic_, as in the construction of the decision tree (decisiontree.sml),
where liveness depends on previous switch choices (nodes above in the decision tree),
i.e. on the "dynamic" path through the decision tree driven by the value being matched.


================================================================================
42. Limiting to essential mvar bindings
================================================================================

The principle is that mvar bindings should only be introduced for "relevant" nodes,
where the mvar will (eventually) be used for either

(1) the value corresponding to a pvar that will need to be supplied to a rule rhs
(2) the value that becomes the subject of a switch.

The idea: introduce bindings for (root or) AND structure nodes only if the 
relevant(id) intersect live is non-empty.  I.e. there is a live (at the point
where the binding occurs) rule that is relevant to the node.  Thus relevance is
qualified or restricted by "liveness" at the point where generate is translating
a node (or its surrounding AND structure).

We can add instrumenttion to check that every mvar that is bound in this way does
eventually get "used".  We can also check that a given (relevant) node is assigned
an mvar once and only once.

Wildcard patterns in themselves will never lead to a binding, though a node with
a Wildcard layer/rule may be mvar bound if there is another source of relevance (i.e.
it is relevant to some other live rule).  Similarly, compound non-relevant patterns
will not yield an mvar binding (e.g. (), (_,_), (_, ()), etc.).

--------------------------------------------------------------------------------
TERMINOLOGY: The "destruct" of a value is the value that result from stipping off
the head constructor, assuming the value's type is a datatype.  For instance,
the destruct of [1] is the pair (1, nil). A datatype constant or other 
constant match element (e.g. an int) does not have a destruct. The destruct of
a vector is a VEC, which is essentially the list of vector elements (the VLENcon
is stripped off).

--------------------------------------------------------------------------------
42.1."Lazy" or "just-in-time" mvar binding

[Editorial comment: How does this relate to relevance analysis?  Does
it depend on it, overlap it, or even supercede it?]

Basic notions:

DEFN: "mvars in scope" or MVS: at a given (andor/dectree) node, what mvars
have been bound and are "visible" or "in scope" at this node?

DEFN: "binding nodeId of an mvar" bindingId(mvar): the andor node where a
variable appears (unique after OR expansion).

DEFN: "nearest-neighbor or nearest-ancestor mvar" at a node is the "closest"
mvar binding to the node.  If we want to introduce a bound mvar to
denote the node value, we should define it via the most direct
relation to one of the nearest-neighbor mvars.

DEFN: "relation of a new mvar binding to an existing bound mvar (probably a
"nearest-neighbor mvar".  We are interested in an mvar that is in
scope and as "close" to this node as possible, where closeness could
be defined in terms of fewest selections needed to define the new mvar
in terms of the existing one. Could also define this in terms of the
shortest chain of selections from the old mvar to the new one.

Do we only need chains of selections (records and/or vector) in
defining a new mvar in terms of an exising bound and in-scope mvar?

Initially we start with a root mvar, unless the whole pattern is
"irrelevant" (e.g. ()). Some "high-level" mvars will be defined in
terms of this top root mvar.  Switch branches (non-degenerate, i.e.
possessing subtree arguments) introduce new local roots.  Then within
these branch subtrees, the local root can serve as an additional
starting point for a definition of local mvars.  So with nested
switches, we get a list of progressively more local definition roots.

DEFN: "lazy" or "just-in-time" mvar binding. This is the principle of
introducing an mvar binding at the latest possible time, or just
before it is going to be used (say in a switch, or maybe just around
a RHS dispatch (RHS function call) to bind its arguments
(corresponding to pvars).

ASSUMPTION: for the source of a definition (the base mvar we are
deriving from) we only use mvars that are "consistent" with the
dectree branch we are currently in.  I.e. we don't need to refer
to mvars that are sub branches not taken on our decision path.

----------------
Example 1

(0)  ((nil, _), true) => 0
(1)  ((x, nil), false) => length x

Andor tree:

   AND 0 {0,1}
     0: AND 1
        0: SWITCH[list] 3 {0,1}
	   nil: {0} $ {0}
	    * : VAR(x) {0,1}
	1: SWITCH[list] 4 {0,1}
	   nil: {1} $ {0,1}
     1: SWITCH[bool]: 2 {0,1}
        true: {0} $ {0}
	false: {1} $ {1}

Here is code for this match that seems to obey the just-in-time
binding discipline:  Here rt is an mvar for the root node of the
andor tree (node 0), rt @ <>.

    let m0 = sel(rt,1)                  -- m0 = rt.1
        sw m0                           -- switch[bool] on m0 @ <1>
	  true =>
	    let m1 = sel (rt, 0)        -- m1 = rt.0
	      let m2 = sel (m1, 0)      -- m2 = rt.0.0
	        sw m2                   -- test m2 @ <0,0> for nil
		  nil => rhs(0) ()      -- invoke rhs(0) with unit arg (no variables)
		  * => FAIL             -- nil required for match
          false =>
	    let m3 = sel (rt, 0)        -- m3 = rt.0; same as m1, but m1 not in scope
	      let m4 = sel (m3, 0)      -- m4 = rt.0.0; relevant to pvar x
	        let m5 = sel(m3, 1)     -- m5 = rt.0.1; case relevant [list]
		  sw m5                 -- test m5 @ <0,1> for nil
		    nil => rhs(1) (m4)  -- pass m4, representing x to rhs(1)
		    * => FAIL           -- nil required for match

mvar bindings: 6, redundant bindings rt.0 (2), rt.0.0 (2)

** Oldmc match code = 

  v13 = FN(v14: TYC({}),(I63)0)               -- rhs(0)
    v12 = FN(v3: TYC(TCAP(list, [TV(1,0)])),  -- rhs(1) [v8 = length, external]
             APP(TAPP(v8 [TV(1,0)]), v3))
      v10 = <Match raising function>
        v15 = v7[1]     -- rt.1               -- v7 = rt, i.e. subject of the match
          v16 = v7[0]   -- rt.0
            SWI v15
             of false.v17 =>
                  v18 = v16[1]                -- v18 = rt.0.1
                    SWI v18
                     of nil.v19 =>            -- v19 = "destruct" nil, not used
		          v20 = v16[0]        -- rt.0.0
                            APP(v12, v20)
                      _ => APP(v10, RCD())    -- FAIL
                true.v21 =>
                  v22 = v16[0]                -- v22 = rt.0.0
                    SWI v22
                     of nil.v23 =>            -- v23 = "destruct" nil, not used
		          APP(v13, RCD())
                      _ => APP(v10, RCD())    -- FAIL
  
mvar bindings: 5, redundant bindings rt.0.0 (2)

The main difference is that the bindings of m1 and m3 (to sel(rt,0) =
rt.0) are "lifted" out the top switch branches and merged.  A name for
rt.0 is needed in both switch branches.

---------------------------
42.2 "Eager" binding

If instead of "lazy" binding, (i.e. binding mvars as late as
possible), we bind eagerly, then for Example 1 we get:

    let m0 = sel (rt, 0)               -- m0 @ <0>    rel: var {x}, sw {3,4}
      let m1 = sel (rt, 1)             -- m1 @ <1>    rel: sw {2}
        let m2 = sel (m0, 0)           -- m2 @ <0.0>  rel: var {x}, sw {3}
	  let m3 = sel (m0, 1)	       -- m3 @ <0.1>  rel: sw {4}
            sw m1                      -- switch[bool] on rt.1
	      true =>
		sw m2                  -- switch[list] on rt.0.0 (test for nil)
		  nil => rhs(0) ()     -- invoke rhs(0) with unit arg (no variables)
		  * => FAIL            -- nil required for match
	      false =>
	        sw m3                  -- switch[list] on rt.0.1 (test for nil)
		  nil => rhs(1) (m4)   -- pass m4, representing x to rhs(1)
		  * => FAIL            -- nil required for match
  
mvar bindings: 4, no redundant bindings

Here we completely destruct the top AND structure before doing any switch.

From this one example, it appears that eager binding may be better
than lazy binding, and that oldmc uses something in-between.

42.3 Binding "time" refinement

With var-relevant nodes, early binding seems to be good always,
because we we definitely need that binding for the pvar on the rhs.

For case-relevant nodes, it might pay off to delay binding if the
case analysis is "guarded" by an intervening case such that the
case analysis only occurs in a subset (minority) of the cases of
the intervening case.  So if there are 5 branches and this value will
be tested in only two of them, there will be three branches where the
binding is not used. The question is what is the dynamic distributing
of selection of the "relevant" branches compared with all the
branches. Here move the binding down into the branches may/will have
a cost in code size, but may save time because the binding is
irrelevant to most (dynamically) selected cases.

Without knowing the dynamic distribution of case selections, we could
fall back on a static approximation: what proportion of cases make use
of the bound variable (in which cases is it still relevant).


================================================================================
[NN] Literature
================================================================================

1. Cardelli, "Compiling a Functional Language", L&FP 1984, Section 5: Pattern Matchng
Describes technique credited to MacQueen & Kahn and used in Hope 1980 [check code!].
Rows of patterns broken down into columns .... Heuristics for choosing columns to
analyze next.

2. Augustsson, "Compiling Pattern Matching", ?, 1985
Describes pattern matching as implemented in LML (Lazy ML).

3. Sestoft, "ML Pattern Match Compilation and Partial Evaluation", ?, ?


================================================================================
[0] High level overview of the new match compiler [email to Bob, 2020.5.31]
================================================================================

I am trying to reconstruct the match compilation algorithm from first
principles (after having spend way too much time trying to reverse
engineer the existing (Bill Aitken) code).  The reverse engineering,
while slow and painful, was one way to come to grips with the
essential ideas.  I have been writing lots of notes as I gradually
understand what is going on.  One simple way of thinking of the
problem is that any pattern-matchable type (a type with outer layers
of concrete product/sum structure, plus a few primitive types with
constants allowed in patterns, i.e. int, work, char, string) has an
associated pattern space that can be represented as a (potentially
infinite) tree which is obtained by unrolling the concrete structure
(abstract types and function types are atomic for this purpose).  A
particular pattern is a finite initial segment (prefix-closed set of
paths) in this pattern space, and a set of patterns can be
characterized by the union of the initial segments for the patterns.
This union gives one an AND-OR tree (AND for product nodes, OR for sum
nodes corresponding to datatypes or open patternable types like int,
string, exceptions (with unbounded numbers of alternatives).  For
engineering/algorithmic purposes you need to annotate this AND-OR tree
with additional information (which bits come from which patterns,
which patterns are live or compatible at a given point in the
pattern space, and what kind of defaulting is introduced by
occurrences of variables in the patterns. [This AND-OR representation
evolved from an earlier matrix view of a pattern sequence that has
been used in lots of formulations  I wrote one up in May, 1084 for
the first time.  The formulating the pattern space as a tree was also
partly inspired by Milners Webs note.]

Having constructed the AND-OR tree to concretely represent the pattern
space, the next step is to order the accessible (non-nested) OR
nodes by some heuristic criteria and from this ordering (roughly)
construct a decision tree using all the relevant choices represented
by the OR nodes.  Once you choose an OR node to be the next choice
on a branch of the decision tree, the AND-OR structure beneath its
children (variants), if any, is opened up and introduces new available
OR nodes for including in the decision tree.  When you have used up
the relevant OR nodes, you have the complete decision tree.

The final step is to flatten the decision tree into (pseudo) code
(e.g. abstract syntax or FLINT plambda). You have to deal with types
and type variables if the code is typed.  Another engineering
challenge is to try to avoid duplicating the switch code because of
the branching structure of the decision tree, where an single OR node
in the static AND-OR representation might be replicated under the
branches of another OR node.

There are various optimality criteria that may be used to drive the
building of the decision tree, such as total code size, minimizing the
number of tests (switches), etc.  At the very least, there will be no
redundant tests performed dynamically (any given OR node from the
AND-OR tree will be represented at most once on any branch of the
decision tree).  The underlying fundamental idea is not to forget any
information derived from a test so that the test might need to be
performed again somewhere down the line.

So that is a capsule summary of my current approach.  There are lots
of variations.  For instance, there are backtracking pattern
matchers that same on code size at the expense of having to repeat
tests.

So far, as I said, Ive been reconstructing this from first
principles.  Before writing this up (as a very tardy follow up to
Baudinet and MacQueen, 1985), Ill need to review the literature (see
SML-history/Implementation/pat-match) to see what wheels I have
re-invented.

================================================================================
